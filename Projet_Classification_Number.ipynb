{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet de classification d'image de nombre\n",
    "\n",
    "L'objectif de ce Notebook est de présenter un exmple de classification d'images de nombre écrit à la main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Guillaume.SAGAZAN\\Documents\\Python Scripts\\Cours\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour la partie Affichage\n",
    "writer = SummaryWriter(\"runs\") # indicate the path of sauvegard\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Selectionne l'appareil qui support le \n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500 \n",
    "num_classes = 10 #10 nombres Existes {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    "num_epochs = 2\n",
    "batch_size = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importe toutes les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "plt.show()\n",
    "img_grid = torchvision.utils.make_grid(example_data)\n",
    "writer.add_image('mnist_images', img_grid) #Permet de rajouter un onglet sur le dash bord avec l'image ajouté\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du Neural Network\n",
    "\n",
    "Fully connected, c'est-à-dire que tous les parametres des différentes couches succéssifs sont connectés entre eux. (Il n'y a pas de trou entre les liaisons).\n",
    "\n",
    "Possède une Couche Cachée, c'est-à-dire une couche de parametre entre l'entré et la sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out) # Hidden Layer\n",
    "        out = self.l2(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "writer.add_graph(model, example_data.reshape(-1, 28*28)) #Model et 1 batch of the data\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/1200], Loss: 0.3067\n",
      "Epoch [1/2], Step [200/1200], Loss: 0.2587\n",
      "Epoch [1/2], Step [300/1200], Loss: 0.1549\n",
      "Epoch [1/2], Step [400/1200], Loss: 0.3841\n",
      "Epoch [1/2], Step [500/1200], Loss: 0.3428\n",
      "Epoch [1/2], Step [600/1200], Loss: 0.0888\n",
      "Epoch [1/2], Step [700/1200], Loss: 0.1330\n",
      "Epoch [1/2], Step [800/1200], Loss: 0.1813\n",
      "Epoch [1/2], Step [900/1200], Loss: 0.2802\n",
      "Epoch [1/2], Step [1000/1200], Loss: 0.3590\n",
      "Epoch [1/2], Step [1100/1200], Loss: 0.0600\n",
      "Epoch [1/2], Step [1200/1200], Loss: 0.2530\n",
      "Epoch [2/2], Step [100/1200], Loss: 0.1264\n",
      "Epoch [2/2], Step [200/1200], Loss: 0.0573\n",
      "Epoch [2/2], Step [300/1200], Loss: 0.0840\n",
      "Epoch [2/2], Step [400/1200], Loss: 0.1666\n",
      "Epoch [2/2], Step [500/1200], Loss: 0.1342\n",
      "Epoch [2/2], Step [600/1200], Loss: 0.0458\n",
      "Epoch [2/2], Step [700/1200], Loss: 0.1358\n",
      "Epoch [2/2], Step [800/1200], Loss: 0.0539\n",
      "Epoch [2/2], Step [900/1200], Loss: 0.0722\n",
      "Epoch [2/2], Step [1000/1200], Loss: 0.0605\n",
      "Epoch [2/2], Step [1100/1200], Loss: 0.0280\n",
      "Epoch [2/2], Step [1200/1200], Loss: 0.1915\n"
     ]
    }
   ],
   "source": [
    "# Pour l'affichage\n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute the metrique pour l'affichage\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            # Ajoute à l'affichage\n",
    "            writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps + i)\n",
    "            writer.add_scalar('accuracy', running_correct / 100, epoch * n_total_steps + i)\n",
    "            running_correct = 0\n",
    "            running_loss = 0\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.02 %\n"
     ]
    }
   ],
   "source": [
    "# Affichage\n",
    "tags = []\n",
    "preds = []\n",
    "# Test the model\n",
    "# Durant la phase de teste le gradient n'a pas besoin des calculer car les poids ne sont pas réactualisés\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        class_prediction = [F.softmax(output, dim=0) for output in outputs]\n",
    "        preds.append(class_prediction)\n",
    "        tags.append(predicted)\n",
    "\n",
    "    preds = torch.cat([torch.stack(batch) for batch in preds])\n",
    "    tags = torch.cat(tags)\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')\n",
    "\n",
    "    classes = range(10)\n",
    "    for i in classes:\n",
    "        tags_i = tags == i\n",
    "        preds_i = preds[:, i]\n",
    "        writer.add_pr_curve(str(i), tags_i, preds_i, global_step=0)\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remaque** : \n",
    "\n",
    "Quelques essaient ont été réalisé avec des hyperparametres différents :\n",
    "| Learning Rate (lr) | Batch Size | Accuracy |\n",
    "| ------------------- | ---------- | -------- |\n",
    "| 0.001               | 100        | 97.02%   |\n",
    "| 0.001               | 50         | 97.45%   |\n",
    "| 0.01                | 100        | 96.06%   |\n",
    "| 0.01                | 50         | 95.3%    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1626fc6907b0c8c2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1626fc6907b0c8c2\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 8888;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs --host localhost --port 8888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde et Enregistre un modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save a model - Lazy method\n",
    "PATH_lazy = \"save\\model.pth\" #extension pth **pytorch**\n",
    "torch.save(model, PATH_lazy)\n",
    "\n",
    "## Save a model - recommended method\n",
    "PATH = \"save\\model_other_method.pth\" #extension pth **pytorch**\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0131, -0.0060,  0.0055,  ..., -0.0024,  0.0251, -0.0183],\n",
      "        [ 0.0109, -0.0061, -0.0256,  ..., -0.0321,  0.0340, -0.0242],\n",
      "        [ 0.0292, -0.0063,  0.0015,  ...,  0.0346,  0.0001,  0.0306],\n",
      "        ...,\n",
      "        [ 0.0081, -0.0195, -0.0350,  ..., -0.0326,  0.0046, -0.0304],\n",
      "        [ 0.0229,  0.0340, -0.0228,  ...,  0.0275, -0.0251, -0.0161],\n",
      "        [-0.0296, -0.0029, -0.0193,  ..., -0.0017, -0.0292,  0.0137]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.4591e-02,  7.7572e-03,  1.3674e-01,  1.2107e-02,  2.1249e-03,\n",
      "         1.7189e-02, -1.2886e-02,  1.0807e-01, -3.2020e-02,  9.0025e-02,\n",
      "        -6.6495e-02,  8.7341e-02,  8.1548e-02,  1.1841e-01,  3.5533e-02,\n",
      "         7.1772e-02,  2.5206e-02,  6.0073e-03,  2.3234e-02,  8.1166e-02,\n",
      "        -3.7411e-02,  3.0906e-02, -3.2593e-02, -3.4184e-02,  1.4150e-02,\n",
      "         1.2606e-01,  8.5487e-02, -2.5094e-02,  2.5176e-02,  4.8127e-02,\n",
      "        -1.5499e-02, -5.6894e-02,  1.3171e-02,  7.6585e-02,  1.0251e-02,\n",
      "         6.5045e-02, -5.0345e-02, -3.7063e-02,  6.2477e-02,  5.3895e-03,\n",
      "        -1.4456e-02, -6.0687e-02,  1.5600e-02,  2.4313e-02,  8.7451e-03,\n",
      "        -9.7664e-02,  1.8768e-02, -2.1943e-02,  2.1051e-02,  1.1277e-02,\n",
      "         6.3198e-02, -3.1444e-02, -2.4319e-02,  7.0692e-03,  2.0848e-03,\n",
      "        -1.2779e-02, -1.1721e-02,  3.7774e-02,  7.5619e-02, -2.3206e-03,\n",
      "        -1.4687e-02,  7.9239e-02, -2.3436e-02, -4.6526e-03,  1.8866e-02,\n",
      "         4.7685e-02,  1.3931e-02, -1.2083e-02,  4.3057e-02,  4.4089e-02,\n",
      "        -1.3680e-02,  4.4813e-02, -8.9600e-03, -2.8521e-03,  1.4434e-02,\n",
      "         7.8451e-02,  5.9873e-02,  2.5925e-02, -5.6943e-02, -2.8477e-03,\n",
      "         1.6367e-02, -4.5586e-02,  1.9205e-02, -4.5119e-02, -5.0903e-02,\n",
      "        -7.5008e-04, -5.0452e-02, -2.0627e-03,  2.8828e-02,  6.6744e-02,\n",
      "        -1.5217e-02, -6.0030e-02,  1.5308e-03,  4.4729e-02,  4.0825e-02,\n",
      "         7.9194e-02, -1.9906e-02, -2.3915e-02,  2.1420e-02,  8.1236e-02,\n",
      "         7.9352e-02, -3.1356e-02, -2.0496e-02,  3.1314e-02,  1.0941e-02,\n",
      "         5.1657e-02, -5.9519e-02, -4.9307e-02, -1.2299e-02,  1.6143e-02,\n",
      "         4.5090e-02,  3.5658e-02, -7.9347e-03,  3.0857e-02,  4.1786e-02,\n",
      "         5.0656e-02,  3.0301e-02,  5.4248e-02, -5.2997e-02,  1.5340e-03,\n",
      "         6.0567e-02,  7.6690e-03,  2.4604e-02, -1.9374e-02,  6.6966e-02,\n",
      "         5.4244e-03,  1.3364e-02,  3.2952e-02,  8.7204e-02, -2.7588e-02,\n",
      "         2.7998e-02, -4.4291e-03,  4.4531e-02,  5.6843e-02,  3.6404e-02,\n",
      "         1.7856e-02,  3.8884e-02,  4.7746e-02, -1.6888e-02,  5.7989e-04,\n",
      "         1.0513e-01,  6.7862e-02,  3.9586e-04, -5.7177e-02,  3.1588e-02,\n",
      "         6.9566e-03,  2.6606e-02,  2.3978e-02,  1.9033e-02, -2.0796e-02,\n",
      "         7.0110e-02,  5.3404e-02,  4.3711e-02, -5.9897e-02,  1.0238e-01,\n",
      "         1.4713e-01,  4.3224e-02,  9.7439e-03,  5.0402e-02, -2.9459e-02,\n",
      "         7.4967e-03,  4.5214e-02, -3.0336e-02, -2.3830e-02, -2.3089e-02,\n",
      "        -1.7171e-02,  1.7742e-02,  6.4822e-02,  5.1205e-02,  7.2254e-03,\n",
      "        -4.3548e-02,  1.7357e-02, -3.1668e-02,  6.8792e-03, -1.0488e-02,\n",
      "         2.4386e-02,  8.6185e-02,  1.8474e-02,  3.4046e-02, -1.9316e-02,\n",
      "         3.7617e-02, -1.1648e-02,  3.1783e-03, -6.5309e-02, -2.2014e-02,\n",
      "         6.5190e-02, -3.8561e-02,  2.0517e-02, -3.7060e-02,  3.7328e-02,\n",
      "         6.5971e-02,  4.3789e-02, -4.9392e-02,  2.6831e-02, -6.6532e-02,\n",
      "         7.9563e-02, -2.6068e-02,  8.0976e-02,  3.7437e-02, -6.8518e-02,\n",
      "        -6.2616e-02,  8.2480e-03, -6.6023e-02, -5.8532e-02, -1.8503e-02,\n",
      "        -4.1110e-02, -4.1753e-02, -2.4456e-02, -3.7014e-02,  8.6219e-02,\n",
      "        -3.3284e-03, -6.1638e-03,  6.3745e-02,  1.9149e-02, -3.3573e-02,\n",
      "         4.5923e-02,  3.2035e-02,  7.3009e-03,  2.0923e-03,  2.5585e-02,\n",
      "         1.4328e-03,  5.7735e-02, -3.2435e-03, -4.5059e-02,  8.0982e-02,\n",
      "         2.5035e-03, -4.3381e-02, -1.4318e-02,  1.8944e-03,  8.0838e-02,\n",
      "         5.5237e-02, -2.9903e-02,  5.9626e-02,  2.5644e-02, -4.9813e-02,\n",
      "        -7.2126e-03, -6.0989e-03, -4.5759e-02, -2.9374e-02,  5.6658e-02,\n",
      "         3.1309e-02,  1.1390e-02,  7.4130e-03, -5.1095e-02, -3.3005e-02,\n",
      "         6.8117e-02,  5.8347e-02,  1.2640e-02,  1.7967e-03,  1.7287e-02,\n",
      "        -8.8042e-03, -7.2478e-03,  2.3928e-02,  7.3410e-02,  4.1354e-02,\n",
      "        -7.0750e-02,  5.6089e-02, -5.8437e-02,  1.0218e-01,  1.5556e-02,\n",
      "        -2.7951e-02,  3.7202e-02,  1.6360e-02,  2.0754e-02,  2.9902e-02,\n",
      "         2.1010e-02, -2.3387e-02, -1.2704e-02, -7.8119e-04,  8.2843e-02,\n",
      "         1.9773e-02, -1.9107e-02,  1.4693e-01, -3.6750e-02, -3.1440e-03,\n",
      "        -6.9502e-02, -2.7487e-02,  4.3526e-02, -2.1519e-03,  6.0127e-02,\n",
      "        -2.2213e-03,  3.4802e-02,  5.8497e-02, -6.5351e-02, -6.5458e-02,\n",
      "        -5.5884e-03,  2.0658e-02, -1.2807e-02, -9.5727e-03, -1.8597e-03,\n",
      "         1.8709e-02, -1.5115e-02,  2.0172e-03, -5.6791e-02,  3.0262e-02,\n",
      "         6.2062e-02,  1.1375e-02,  1.7115e-02,  6.8861e-02,  2.1660e-02,\n",
      "         1.7230e-02,  6.3231e-02,  6.8807e-02,  4.7124e-02,  7.7456e-03,\n",
      "         7.2647e-02,  2.3503e-02,  5.6931e-02,  1.1031e-02,  8.2467e-02,\n",
      "         2.6623e-02,  5.7653e-02,  4.3940e-02,  6.8877e-02,  5.3508e-02,\n",
      "         2.4395e-02,  5.0548e-02,  6.9615e-02, -3.8290e-02,  1.7803e-02,\n",
      "         2.3130e-02,  1.1033e-01, -1.4699e-02,  1.8934e-02,  1.7860e-02,\n",
      "         1.2969e-01, -5.6704e-03, -1.4962e-02,  2.7848e-02,  3.2184e-02,\n",
      "         1.7244e-02, -4.5284e-02,  2.9843e-04, -1.9973e-02, -3.0584e-02,\n",
      "         3.6722e-02, -4.2787e-03,  6.6666e-02,  1.0068e-01, -4.1064e-03,\n",
      "         6.8306e-02,  2.1863e-02,  2.4445e-02,  6.0837e-03,  9.9320e-03,\n",
      "         4.7288e-02, -3.4952e-02,  2.6490e-02,  9.7792e-02, -4.8034e-02,\n",
      "        -1.4175e-03,  1.3867e-02, -2.4006e-02,  6.3439e-02,  5.6167e-02,\n",
      "        -4.8859e-02,  5.3069e-02,  4.4348e-02, -4.9945e-02,  6.0582e-02,\n",
      "         1.5613e-02,  4.3854e-02, -3.0551e-02, -5.3640e-02,  3.5375e-02,\n",
      "        -2.5779e-02, -7.3098e-02, -9.8592e-03,  5.9661e-03,  4.1880e-02,\n",
      "        -2.1723e-02,  6.8703e-02, -5.6582e-02,  7.7288e-02,  3.4325e-02,\n",
      "        -4.2844e-03,  3.7072e-02,  2.4162e-02,  8.5114e-03,  3.5648e-02,\n",
      "         1.0285e-01,  1.4486e-02,  2.1277e-02,  1.7669e-02,  9.5676e-02,\n",
      "        -1.9534e-02,  5.6891e-02,  5.3008e-02,  8.9383e-03,  1.4656e-02,\n",
      "         6.4487e-02,  6.4689e-02, -4.2927e-02,  2.2167e-02, -3.2769e-02,\n",
      "         1.4171e-02, -2.8052e-02, -5.4986e-02, -2.9767e-02,  4.6405e-02,\n",
      "         4.7703e-03, -2.1947e-02,  1.1731e-01,  7.1213e-02, -1.1055e-02,\n",
      "        -1.7396e-02,  4.1562e-03,  3.1343e-03,  2.2696e-02,  8.9470e-05,\n",
      "         8.1240e-02, -2.1137e-03, -1.8582e-03,  7.0409e-02,  6.1504e-02,\n",
      "         2.4979e-02, -2.2549e-02,  3.7724e-04, -3.4474e-02, -2.1587e-02,\n",
      "         5.6500e-02, -6.3841e-03,  3.8372e-02,  1.0857e-02,  2.5586e-02,\n",
      "         5.1662e-02,  1.1973e-01,  1.2452e-01, -1.5986e-02, -1.2709e-02,\n",
      "        -7.9307e-02,  1.7113e-02, -7.5556e-03, -5.1539e-02, -2.2366e-03,\n",
      "        -5.0329e-03,  1.2269e-02,  2.9231e-02,  2.0337e-02,  1.0385e-02,\n",
      "        -1.1502e-02,  5.6884e-02,  4.9987e-02, -3.7130e-02,  3.1793e-02,\n",
      "        -1.4150e-03,  1.7134e-02, -1.5081e-02,  1.2790e-03,  4.3094e-02,\n",
      "        -2.4927e-02, -9.5608e-03, -2.4214e-02,  2.7892e-02,  1.7139e-02,\n",
      "         1.3838e-02,  5.7702e-03,  5.5814e-02, -3.8876e-02, -5.9479e-02,\n",
      "         6.1695e-02,  2.5853e-02,  1.1102e-01, -3.5087e-02,  6.3513e-02,\n",
      "         2.3395e-02, -5.2042e-02,  4.6997e-02,  2.7355e-02,  1.0300e-01,\n",
      "         2.8223e-02, -1.1131e-03,  6.6912e-02, -3.3925e-02,  2.0122e-03,\n",
      "         3.1292e-02, -7.8008e-03,  3.2552e-02, -5.5640e-02, -7.2654e-02,\n",
      "        -6.2286e-02, -1.0083e-02,  5.1079e-02,  7.2529e-03, -2.2576e-02,\n",
      "         7.0860e-02, -1.7032e-02, -6.0404e-03,  8.4726e-02, -5.7115e-03,\n",
      "        -1.0194e-02,  8.4117e-03, -1.5191e-02, -1.8968e-02, -9.4096e-03,\n",
      "         3.8501e-02,  8.7042e-02,  6.2513e-02,  1.3268e-02,  3.3550e-02],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0180, -0.0959, -0.2056,  ...,  0.0764, -0.0765, -0.0441],\n",
      "        [-0.0920,  0.2689, -0.0132,  ..., -0.1096,  0.0250, -0.0775],\n",
      "        [ 0.1016, -0.0737, -0.2551,  ...,  0.0495,  0.0543, -0.0527],\n",
      "        ...,\n",
      "        [-0.0312,  0.2245,  0.0353,  ..., -0.0442, -0.1056,  0.1107],\n",
      "        [-0.0788, -0.2211, -0.0197,  ...,  0.0314,  0.0387, -0.0421],\n",
      "        [-0.1122, -0.0584,  0.0726,  ..., -0.0461,  0.0089, -0.0652]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0321,  0.0413,  0.0144, -0.0363,  0.0350,  0.0533, -0.0068, -0.0450,\n",
      "        -0.0157, -0.0545], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0131, -0.0060,  0.0055,  ..., -0.0024,  0.0251, -0.0183],\n",
      "        [ 0.0109, -0.0061, -0.0256,  ..., -0.0321,  0.0340, -0.0242],\n",
      "        [ 0.0292, -0.0063,  0.0015,  ...,  0.0346,  0.0001,  0.0306],\n",
      "        ...,\n",
      "        [ 0.0081, -0.0195, -0.0350,  ..., -0.0326,  0.0046, -0.0304],\n",
      "        [ 0.0229,  0.0340, -0.0228,  ...,  0.0275, -0.0251, -0.0161],\n",
      "        [-0.0296, -0.0029, -0.0193,  ..., -0.0017, -0.0292,  0.0137]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.4591e-02,  7.7572e-03,  1.3674e-01,  1.2107e-02,  2.1249e-03,\n",
      "         1.7189e-02, -1.2886e-02,  1.0807e-01, -3.2020e-02,  9.0025e-02,\n",
      "        -6.6495e-02,  8.7341e-02,  8.1548e-02,  1.1841e-01,  3.5533e-02,\n",
      "         7.1772e-02,  2.5206e-02,  6.0073e-03,  2.3234e-02,  8.1166e-02,\n",
      "        -3.7411e-02,  3.0906e-02, -3.2593e-02, -3.4184e-02,  1.4150e-02,\n",
      "         1.2606e-01,  8.5487e-02, -2.5094e-02,  2.5176e-02,  4.8127e-02,\n",
      "        -1.5499e-02, -5.6894e-02,  1.3171e-02,  7.6585e-02,  1.0251e-02,\n",
      "         6.5045e-02, -5.0345e-02, -3.7063e-02,  6.2477e-02,  5.3895e-03,\n",
      "        -1.4456e-02, -6.0687e-02,  1.5600e-02,  2.4313e-02,  8.7451e-03,\n",
      "        -9.7664e-02,  1.8768e-02, -2.1943e-02,  2.1051e-02,  1.1277e-02,\n",
      "         6.3198e-02, -3.1444e-02, -2.4319e-02,  7.0692e-03,  2.0848e-03,\n",
      "        -1.2779e-02, -1.1721e-02,  3.7774e-02,  7.5619e-02, -2.3206e-03,\n",
      "        -1.4687e-02,  7.9239e-02, -2.3436e-02, -4.6526e-03,  1.8866e-02,\n",
      "         4.7685e-02,  1.3931e-02, -1.2083e-02,  4.3057e-02,  4.4089e-02,\n",
      "        -1.3680e-02,  4.4813e-02, -8.9600e-03, -2.8521e-03,  1.4434e-02,\n",
      "         7.8451e-02,  5.9873e-02,  2.5925e-02, -5.6943e-02, -2.8477e-03,\n",
      "         1.6367e-02, -4.5586e-02,  1.9205e-02, -4.5119e-02, -5.0903e-02,\n",
      "        -7.5008e-04, -5.0452e-02, -2.0627e-03,  2.8828e-02,  6.6744e-02,\n",
      "        -1.5217e-02, -6.0030e-02,  1.5308e-03,  4.4729e-02,  4.0825e-02,\n",
      "         7.9194e-02, -1.9906e-02, -2.3915e-02,  2.1420e-02,  8.1236e-02,\n",
      "         7.9352e-02, -3.1356e-02, -2.0496e-02,  3.1314e-02,  1.0941e-02,\n",
      "         5.1657e-02, -5.9519e-02, -4.9307e-02, -1.2299e-02,  1.6143e-02,\n",
      "         4.5090e-02,  3.5658e-02, -7.9347e-03,  3.0857e-02,  4.1786e-02,\n",
      "         5.0656e-02,  3.0301e-02,  5.4248e-02, -5.2997e-02,  1.5340e-03,\n",
      "         6.0567e-02,  7.6690e-03,  2.4604e-02, -1.9374e-02,  6.6966e-02,\n",
      "         5.4244e-03,  1.3364e-02,  3.2952e-02,  8.7204e-02, -2.7588e-02,\n",
      "         2.7998e-02, -4.4291e-03,  4.4531e-02,  5.6843e-02,  3.6404e-02,\n",
      "         1.7856e-02,  3.8884e-02,  4.7746e-02, -1.6888e-02,  5.7989e-04,\n",
      "         1.0513e-01,  6.7862e-02,  3.9586e-04, -5.7177e-02,  3.1588e-02,\n",
      "         6.9566e-03,  2.6606e-02,  2.3978e-02,  1.9033e-02, -2.0796e-02,\n",
      "         7.0110e-02,  5.3404e-02,  4.3711e-02, -5.9897e-02,  1.0238e-01,\n",
      "         1.4713e-01,  4.3224e-02,  9.7439e-03,  5.0402e-02, -2.9459e-02,\n",
      "         7.4967e-03,  4.5214e-02, -3.0336e-02, -2.3830e-02, -2.3089e-02,\n",
      "        -1.7171e-02,  1.7742e-02,  6.4822e-02,  5.1205e-02,  7.2254e-03,\n",
      "        -4.3548e-02,  1.7357e-02, -3.1668e-02,  6.8792e-03, -1.0488e-02,\n",
      "         2.4386e-02,  8.6185e-02,  1.8474e-02,  3.4046e-02, -1.9316e-02,\n",
      "         3.7617e-02, -1.1648e-02,  3.1783e-03, -6.5309e-02, -2.2014e-02,\n",
      "         6.5190e-02, -3.8561e-02,  2.0517e-02, -3.7060e-02,  3.7328e-02,\n",
      "         6.5971e-02,  4.3789e-02, -4.9392e-02,  2.6831e-02, -6.6532e-02,\n",
      "         7.9563e-02, -2.6068e-02,  8.0976e-02,  3.7437e-02, -6.8518e-02,\n",
      "        -6.2616e-02,  8.2480e-03, -6.6023e-02, -5.8532e-02, -1.8503e-02,\n",
      "        -4.1110e-02, -4.1753e-02, -2.4456e-02, -3.7014e-02,  8.6219e-02,\n",
      "        -3.3284e-03, -6.1638e-03,  6.3745e-02,  1.9149e-02, -3.3573e-02,\n",
      "         4.5923e-02,  3.2035e-02,  7.3009e-03,  2.0923e-03,  2.5585e-02,\n",
      "         1.4328e-03,  5.7735e-02, -3.2435e-03, -4.5059e-02,  8.0982e-02,\n",
      "         2.5035e-03, -4.3381e-02, -1.4318e-02,  1.8944e-03,  8.0838e-02,\n",
      "         5.5237e-02, -2.9903e-02,  5.9626e-02,  2.5644e-02, -4.9813e-02,\n",
      "        -7.2126e-03, -6.0989e-03, -4.5759e-02, -2.9374e-02,  5.6658e-02,\n",
      "         3.1309e-02,  1.1390e-02,  7.4130e-03, -5.1095e-02, -3.3005e-02,\n",
      "         6.8117e-02,  5.8347e-02,  1.2640e-02,  1.7967e-03,  1.7287e-02,\n",
      "        -8.8042e-03, -7.2478e-03,  2.3928e-02,  7.3410e-02,  4.1354e-02,\n",
      "        -7.0750e-02,  5.6089e-02, -5.8437e-02,  1.0218e-01,  1.5556e-02,\n",
      "        -2.7951e-02,  3.7202e-02,  1.6360e-02,  2.0754e-02,  2.9902e-02,\n",
      "         2.1010e-02, -2.3387e-02, -1.2704e-02, -7.8119e-04,  8.2843e-02,\n",
      "         1.9773e-02, -1.9107e-02,  1.4693e-01, -3.6750e-02, -3.1440e-03,\n",
      "        -6.9502e-02, -2.7487e-02,  4.3526e-02, -2.1519e-03,  6.0127e-02,\n",
      "        -2.2213e-03,  3.4802e-02,  5.8497e-02, -6.5351e-02, -6.5458e-02,\n",
      "        -5.5884e-03,  2.0658e-02, -1.2807e-02, -9.5727e-03, -1.8597e-03,\n",
      "         1.8709e-02, -1.5115e-02,  2.0172e-03, -5.6791e-02,  3.0262e-02,\n",
      "         6.2062e-02,  1.1375e-02,  1.7115e-02,  6.8861e-02,  2.1660e-02,\n",
      "         1.7230e-02,  6.3231e-02,  6.8807e-02,  4.7124e-02,  7.7456e-03,\n",
      "         7.2647e-02,  2.3503e-02,  5.6931e-02,  1.1031e-02,  8.2467e-02,\n",
      "         2.6623e-02,  5.7653e-02,  4.3940e-02,  6.8877e-02,  5.3508e-02,\n",
      "         2.4395e-02,  5.0548e-02,  6.9615e-02, -3.8290e-02,  1.7803e-02,\n",
      "         2.3130e-02,  1.1033e-01, -1.4699e-02,  1.8934e-02,  1.7860e-02,\n",
      "         1.2969e-01, -5.6704e-03, -1.4962e-02,  2.7848e-02,  3.2184e-02,\n",
      "         1.7244e-02, -4.5284e-02,  2.9843e-04, -1.9973e-02, -3.0584e-02,\n",
      "         3.6722e-02, -4.2787e-03,  6.6666e-02,  1.0068e-01, -4.1064e-03,\n",
      "         6.8306e-02,  2.1863e-02,  2.4445e-02,  6.0837e-03,  9.9320e-03,\n",
      "         4.7288e-02, -3.4952e-02,  2.6490e-02,  9.7792e-02, -4.8034e-02,\n",
      "        -1.4175e-03,  1.3867e-02, -2.4006e-02,  6.3439e-02,  5.6167e-02,\n",
      "        -4.8859e-02,  5.3069e-02,  4.4348e-02, -4.9945e-02,  6.0582e-02,\n",
      "         1.5613e-02,  4.3854e-02, -3.0551e-02, -5.3640e-02,  3.5375e-02,\n",
      "        -2.5779e-02, -7.3098e-02, -9.8592e-03,  5.9661e-03,  4.1880e-02,\n",
      "        -2.1723e-02,  6.8703e-02, -5.6582e-02,  7.7288e-02,  3.4325e-02,\n",
      "        -4.2844e-03,  3.7072e-02,  2.4162e-02,  8.5114e-03,  3.5648e-02,\n",
      "         1.0285e-01,  1.4486e-02,  2.1277e-02,  1.7669e-02,  9.5676e-02,\n",
      "        -1.9534e-02,  5.6891e-02,  5.3008e-02,  8.9383e-03,  1.4656e-02,\n",
      "         6.4487e-02,  6.4689e-02, -4.2927e-02,  2.2167e-02, -3.2769e-02,\n",
      "         1.4171e-02, -2.8052e-02, -5.4986e-02, -2.9767e-02,  4.6405e-02,\n",
      "         4.7703e-03, -2.1947e-02,  1.1731e-01,  7.1213e-02, -1.1055e-02,\n",
      "        -1.7396e-02,  4.1562e-03,  3.1343e-03,  2.2696e-02,  8.9470e-05,\n",
      "         8.1240e-02, -2.1137e-03, -1.8582e-03,  7.0409e-02,  6.1504e-02,\n",
      "         2.4979e-02, -2.2549e-02,  3.7724e-04, -3.4474e-02, -2.1587e-02,\n",
      "         5.6500e-02, -6.3841e-03,  3.8372e-02,  1.0857e-02,  2.5586e-02,\n",
      "         5.1662e-02,  1.1973e-01,  1.2452e-01, -1.5986e-02, -1.2709e-02,\n",
      "        -7.9307e-02,  1.7113e-02, -7.5556e-03, -5.1539e-02, -2.2366e-03,\n",
      "        -5.0329e-03,  1.2269e-02,  2.9231e-02,  2.0337e-02,  1.0385e-02,\n",
      "        -1.1502e-02,  5.6884e-02,  4.9987e-02, -3.7130e-02,  3.1793e-02,\n",
      "        -1.4150e-03,  1.7134e-02, -1.5081e-02,  1.2790e-03,  4.3094e-02,\n",
      "        -2.4927e-02, -9.5608e-03, -2.4214e-02,  2.7892e-02,  1.7139e-02,\n",
      "         1.3838e-02,  5.7702e-03,  5.5814e-02, -3.8876e-02, -5.9479e-02,\n",
      "         6.1695e-02,  2.5853e-02,  1.1102e-01, -3.5087e-02,  6.3513e-02,\n",
      "         2.3395e-02, -5.2042e-02,  4.6997e-02,  2.7355e-02,  1.0300e-01,\n",
      "         2.8223e-02, -1.1131e-03,  6.6912e-02, -3.3925e-02,  2.0122e-03,\n",
      "         3.1292e-02, -7.8008e-03,  3.2552e-02, -5.5640e-02, -7.2654e-02,\n",
      "        -6.2286e-02, -1.0083e-02,  5.1079e-02,  7.2529e-03, -2.2576e-02,\n",
      "         7.0860e-02, -1.7032e-02, -6.0404e-03,  8.4726e-02, -5.7115e-03,\n",
      "        -1.0194e-02,  8.4117e-03, -1.5191e-02, -1.8968e-02, -9.4096e-03,\n",
      "         3.8501e-02,  8.7042e-02,  6.2513e-02,  1.3268e-02,  3.3550e-02],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0180, -0.0959, -0.2056,  ...,  0.0764, -0.0765, -0.0441],\n",
      "        [-0.0920,  0.2689, -0.0132,  ..., -0.1096,  0.0250, -0.0775],\n",
      "        [ 0.1016, -0.0737, -0.2551,  ...,  0.0495,  0.0543, -0.0527],\n",
      "        ...,\n",
      "        [-0.0312,  0.2245,  0.0353,  ..., -0.0442, -0.1056,  0.1107],\n",
      "        [-0.0788, -0.2211, -0.0197,  ...,  0.0314,  0.0387, -0.0421],\n",
      "        [-0.1122, -0.0584,  0.0726,  ..., -0.0461,  0.0089, -0.0652]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0321,  0.0413,  0.0144, -0.0363,  0.0350,  0.0533, -0.0068, -0.0450,\n",
      "        -0.0157, -0.0545], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## load model - Lazy method\n",
    "model_load = torch.load(PATH_lazy)\n",
    "model_load.eval()\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "## load model - Recommended method\n",
    "loaded_model = NeuralNet(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_classes=num_classes\n",
    "    )\n",
    "loaded_model.load_state_dict(\n",
    "    torch.load(PATH)\n",
    "    )\n",
    "loaded_model.eval() \n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('l1.weight', tensor([[-0.0131, -0.0060,  0.0055,  ..., -0.0024,  0.0251, -0.0183],\n",
      "        [ 0.0109, -0.0061, -0.0256,  ..., -0.0321,  0.0340, -0.0242],\n",
      "        [ 0.0292, -0.0063,  0.0015,  ...,  0.0346,  0.0001,  0.0306],\n",
      "        ...,\n",
      "        [ 0.0081, -0.0195, -0.0350,  ..., -0.0326,  0.0046, -0.0304],\n",
      "        [ 0.0229,  0.0340, -0.0228,  ...,  0.0275, -0.0251, -0.0161],\n",
      "        [-0.0296, -0.0029, -0.0193,  ..., -0.0017, -0.0292,  0.0137]])), ('l1.bias', tensor([-1.4591e-02,  7.7572e-03,  1.3674e-01,  1.2107e-02,  2.1249e-03,\n",
      "         1.7189e-02, -1.2886e-02,  1.0807e-01, -3.2020e-02,  9.0025e-02,\n",
      "        -6.6495e-02,  8.7341e-02,  8.1548e-02,  1.1841e-01,  3.5533e-02,\n",
      "         7.1772e-02,  2.5206e-02,  6.0073e-03,  2.3234e-02,  8.1166e-02,\n",
      "        -3.7411e-02,  3.0906e-02, -3.2593e-02, -3.4184e-02,  1.4150e-02,\n",
      "         1.2606e-01,  8.5487e-02, -2.5094e-02,  2.5176e-02,  4.8127e-02,\n",
      "        -1.5499e-02, -5.6894e-02,  1.3171e-02,  7.6585e-02,  1.0251e-02,\n",
      "         6.5045e-02, -5.0345e-02, -3.7063e-02,  6.2477e-02,  5.3895e-03,\n",
      "        -1.4456e-02, -6.0687e-02,  1.5600e-02,  2.4313e-02,  8.7451e-03,\n",
      "        -9.7664e-02,  1.8768e-02, -2.1943e-02,  2.1051e-02,  1.1277e-02,\n",
      "         6.3198e-02, -3.1444e-02, -2.4319e-02,  7.0692e-03,  2.0848e-03,\n",
      "        -1.2779e-02, -1.1721e-02,  3.7774e-02,  7.5619e-02, -2.3206e-03,\n",
      "        -1.4687e-02,  7.9239e-02, -2.3436e-02, -4.6526e-03,  1.8866e-02,\n",
      "         4.7685e-02,  1.3931e-02, -1.2083e-02,  4.3057e-02,  4.4089e-02,\n",
      "        -1.3680e-02,  4.4813e-02, -8.9600e-03, -2.8521e-03,  1.4434e-02,\n",
      "         7.8451e-02,  5.9873e-02,  2.5925e-02, -5.6943e-02, -2.8477e-03,\n",
      "         1.6367e-02, -4.5586e-02,  1.9205e-02, -4.5119e-02, -5.0903e-02,\n",
      "        -7.5008e-04, -5.0452e-02, -2.0627e-03,  2.8828e-02,  6.6744e-02,\n",
      "        -1.5217e-02, -6.0030e-02,  1.5308e-03,  4.4729e-02,  4.0825e-02,\n",
      "         7.9194e-02, -1.9906e-02, -2.3915e-02,  2.1420e-02,  8.1236e-02,\n",
      "         7.9352e-02, -3.1356e-02, -2.0496e-02,  3.1314e-02,  1.0941e-02,\n",
      "         5.1657e-02, -5.9519e-02, -4.9307e-02, -1.2299e-02,  1.6143e-02,\n",
      "         4.5090e-02,  3.5658e-02, -7.9347e-03,  3.0857e-02,  4.1786e-02,\n",
      "         5.0656e-02,  3.0301e-02,  5.4248e-02, -5.2997e-02,  1.5340e-03,\n",
      "         6.0567e-02,  7.6690e-03,  2.4604e-02, -1.9374e-02,  6.6966e-02,\n",
      "         5.4244e-03,  1.3364e-02,  3.2952e-02,  8.7204e-02, -2.7588e-02,\n",
      "         2.7998e-02, -4.4291e-03,  4.4531e-02,  5.6843e-02,  3.6404e-02,\n",
      "         1.7856e-02,  3.8884e-02,  4.7746e-02, -1.6888e-02,  5.7989e-04,\n",
      "         1.0513e-01,  6.7862e-02,  3.9586e-04, -5.7177e-02,  3.1588e-02,\n",
      "         6.9566e-03,  2.6606e-02,  2.3978e-02,  1.9033e-02, -2.0796e-02,\n",
      "         7.0110e-02,  5.3404e-02,  4.3711e-02, -5.9897e-02,  1.0238e-01,\n",
      "         1.4713e-01,  4.3224e-02,  9.7439e-03,  5.0402e-02, -2.9459e-02,\n",
      "         7.4967e-03,  4.5214e-02, -3.0336e-02, -2.3830e-02, -2.3089e-02,\n",
      "        -1.7171e-02,  1.7742e-02,  6.4822e-02,  5.1205e-02,  7.2254e-03,\n",
      "        -4.3548e-02,  1.7357e-02, -3.1668e-02,  6.8792e-03, -1.0488e-02,\n",
      "         2.4386e-02,  8.6185e-02,  1.8474e-02,  3.4046e-02, -1.9316e-02,\n",
      "         3.7617e-02, -1.1648e-02,  3.1783e-03, -6.5309e-02, -2.2014e-02,\n",
      "         6.5190e-02, -3.8561e-02,  2.0517e-02, -3.7060e-02,  3.7328e-02,\n",
      "         6.5971e-02,  4.3789e-02, -4.9392e-02,  2.6831e-02, -6.6532e-02,\n",
      "         7.9563e-02, -2.6068e-02,  8.0976e-02,  3.7437e-02, -6.8518e-02,\n",
      "        -6.2616e-02,  8.2480e-03, -6.6023e-02, -5.8532e-02, -1.8503e-02,\n",
      "        -4.1110e-02, -4.1753e-02, -2.4456e-02, -3.7014e-02,  8.6219e-02,\n",
      "        -3.3284e-03, -6.1638e-03,  6.3745e-02,  1.9149e-02, -3.3573e-02,\n",
      "         4.5923e-02,  3.2035e-02,  7.3009e-03,  2.0923e-03,  2.5585e-02,\n",
      "         1.4328e-03,  5.7735e-02, -3.2435e-03, -4.5059e-02,  8.0982e-02,\n",
      "         2.5035e-03, -4.3381e-02, -1.4318e-02,  1.8944e-03,  8.0838e-02,\n",
      "         5.5237e-02, -2.9903e-02,  5.9626e-02,  2.5644e-02, -4.9813e-02,\n",
      "        -7.2126e-03, -6.0989e-03, -4.5759e-02, -2.9374e-02,  5.6658e-02,\n",
      "         3.1309e-02,  1.1390e-02,  7.4130e-03, -5.1095e-02, -3.3005e-02,\n",
      "         6.8117e-02,  5.8347e-02,  1.2640e-02,  1.7967e-03,  1.7287e-02,\n",
      "        -8.8042e-03, -7.2478e-03,  2.3928e-02,  7.3410e-02,  4.1354e-02,\n",
      "        -7.0750e-02,  5.6089e-02, -5.8437e-02,  1.0218e-01,  1.5556e-02,\n",
      "        -2.7951e-02,  3.7202e-02,  1.6360e-02,  2.0754e-02,  2.9902e-02,\n",
      "         2.1010e-02, -2.3387e-02, -1.2704e-02, -7.8119e-04,  8.2843e-02,\n",
      "         1.9773e-02, -1.9107e-02,  1.4693e-01, -3.6750e-02, -3.1440e-03,\n",
      "        -6.9502e-02, -2.7487e-02,  4.3526e-02, -2.1519e-03,  6.0127e-02,\n",
      "        -2.2213e-03,  3.4802e-02,  5.8497e-02, -6.5351e-02, -6.5458e-02,\n",
      "        -5.5884e-03,  2.0658e-02, -1.2807e-02, -9.5727e-03, -1.8597e-03,\n",
      "         1.8709e-02, -1.5115e-02,  2.0172e-03, -5.6791e-02,  3.0262e-02,\n",
      "         6.2062e-02,  1.1375e-02,  1.7115e-02,  6.8861e-02,  2.1660e-02,\n",
      "         1.7230e-02,  6.3231e-02,  6.8807e-02,  4.7124e-02,  7.7456e-03,\n",
      "         7.2647e-02,  2.3503e-02,  5.6931e-02,  1.1031e-02,  8.2467e-02,\n",
      "         2.6623e-02,  5.7653e-02,  4.3940e-02,  6.8877e-02,  5.3508e-02,\n",
      "         2.4395e-02,  5.0548e-02,  6.9615e-02, -3.8290e-02,  1.7803e-02,\n",
      "         2.3130e-02,  1.1033e-01, -1.4699e-02,  1.8934e-02,  1.7860e-02,\n",
      "         1.2969e-01, -5.6704e-03, -1.4962e-02,  2.7848e-02,  3.2184e-02,\n",
      "         1.7244e-02, -4.5284e-02,  2.9843e-04, -1.9973e-02, -3.0584e-02,\n",
      "         3.6722e-02, -4.2787e-03,  6.6666e-02,  1.0068e-01, -4.1064e-03,\n",
      "         6.8306e-02,  2.1863e-02,  2.4445e-02,  6.0837e-03,  9.9320e-03,\n",
      "         4.7288e-02, -3.4952e-02,  2.6490e-02,  9.7792e-02, -4.8034e-02,\n",
      "        -1.4175e-03,  1.3867e-02, -2.4006e-02,  6.3439e-02,  5.6167e-02,\n",
      "        -4.8859e-02,  5.3069e-02,  4.4348e-02, -4.9945e-02,  6.0582e-02,\n",
      "         1.5613e-02,  4.3854e-02, -3.0551e-02, -5.3640e-02,  3.5375e-02,\n",
      "        -2.5779e-02, -7.3098e-02, -9.8592e-03,  5.9661e-03,  4.1880e-02,\n",
      "        -2.1723e-02,  6.8703e-02, -5.6582e-02,  7.7288e-02,  3.4325e-02,\n",
      "        -4.2844e-03,  3.7072e-02,  2.4162e-02,  8.5114e-03,  3.5648e-02,\n",
      "         1.0285e-01,  1.4486e-02,  2.1277e-02,  1.7669e-02,  9.5676e-02,\n",
      "        -1.9534e-02,  5.6891e-02,  5.3008e-02,  8.9383e-03,  1.4656e-02,\n",
      "         6.4487e-02,  6.4689e-02, -4.2927e-02,  2.2167e-02, -3.2769e-02,\n",
      "         1.4171e-02, -2.8052e-02, -5.4986e-02, -2.9767e-02,  4.6405e-02,\n",
      "         4.7703e-03, -2.1947e-02,  1.1731e-01,  7.1213e-02, -1.1055e-02,\n",
      "        -1.7396e-02,  4.1562e-03,  3.1343e-03,  2.2696e-02,  8.9470e-05,\n",
      "         8.1240e-02, -2.1137e-03, -1.8582e-03,  7.0409e-02,  6.1504e-02,\n",
      "         2.4979e-02, -2.2549e-02,  3.7724e-04, -3.4474e-02, -2.1587e-02,\n",
      "         5.6500e-02, -6.3841e-03,  3.8372e-02,  1.0857e-02,  2.5586e-02,\n",
      "         5.1662e-02,  1.1973e-01,  1.2452e-01, -1.5986e-02, -1.2709e-02,\n",
      "        -7.9307e-02,  1.7113e-02, -7.5556e-03, -5.1539e-02, -2.2366e-03,\n",
      "        -5.0329e-03,  1.2269e-02,  2.9231e-02,  2.0337e-02,  1.0385e-02,\n",
      "        -1.1502e-02,  5.6884e-02,  4.9987e-02, -3.7130e-02,  3.1793e-02,\n",
      "        -1.4150e-03,  1.7134e-02, -1.5081e-02,  1.2790e-03,  4.3094e-02,\n",
      "        -2.4927e-02, -9.5608e-03, -2.4214e-02,  2.7892e-02,  1.7139e-02,\n",
      "         1.3838e-02,  5.7702e-03,  5.5814e-02, -3.8876e-02, -5.9479e-02,\n",
      "         6.1695e-02,  2.5853e-02,  1.1102e-01, -3.5087e-02,  6.3513e-02,\n",
      "         2.3395e-02, -5.2042e-02,  4.6997e-02,  2.7355e-02,  1.0300e-01,\n",
      "         2.8223e-02, -1.1131e-03,  6.6912e-02, -3.3925e-02,  2.0122e-03,\n",
      "         3.1292e-02, -7.8008e-03,  3.2552e-02, -5.5640e-02, -7.2654e-02,\n",
      "        -6.2286e-02, -1.0083e-02,  5.1079e-02,  7.2529e-03, -2.2576e-02,\n",
      "         7.0860e-02, -1.7032e-02, -6.0404e-03,  8.4726e-02, -5.7115e-03,\n",
      "        -1.0194e-02,  8.4117e-03, -1.5191e-02, -1.8968e-02, -9.4096e-03,\n",
      "         3.8501e-02,  8.7042e-02,  6.2513e-02,  1.3268e-02,  3.3550e-02])), ('l2.weight', tensor([[ 0.0180, -0.0959, -0.2056,  ...,  0.0764, -0.0765, -0.0441],\n",
      "        [-0.0920,  0.2689, -0.0132,  ..., -0.1096,  0.0250, -0.0775],\n",
      "        [ 0.1016, -0.0737, -0.2551,  ...,  0.0495,  0.0543, -0.0527],\n",
      "        ...,\n",
      "        [-0.0312,  0.2245,  0.0353,  ..., -0.0442, -0.1056,  0.1107],\n",
      "        [-0.0788, -0.2211, -0.0197,  ...,  0.0314,  0.0387, -0.0421],\n",
      "        [-0.1122, -0.0584,  0.0726,  ..., -0.0461,  0.0089, -0.0652]])), ('l2.bias', tensor([-0.0321,  0.0413,  0.0144, -0.0363,  0.0350,  0.0533, -0.0068, -0.0450,\n",
      "        -0.0157, -0.0545]))])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {0: {'step': tensor(2400.), 'exp_avg': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'exp_avg_sq': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}, 1: {'step': tensor(2400.), 'exp_avg': tensor([-9.3249e-05,  7.5698e-05, -4.8452e-04, -5.0895e-04,  8.7830e-05,\n",
      "        -1.3093e-03, -5.5164e-05, -3.2693e-05, -3.5481e-05,  2.6537e-04,\n",
      "         3.3826e-04,  9.0369e-04, -1.1897e-04,  6.1029e-04,  5.7548e-04,\n",
      "         1.8348e-04,  3.7803e-04, -6.4731e-04,  1.8818e-04,  2.6265e-05,\n",
      "         1.5586e-05,  4.4438e-04, -3.7537e-05,  3.2936e-04, -1.2720e-04,\n",
      "        -5.5299e-05,  5.3731e-04,  3.9039e-04,  6.9184e-04, -2.8085e-04,\n",
      "         2.9519e-04, -1.7855e-04, -1.9896e-04,  2.7557e-04,  1.8187e-04,\n",
      "        -2.5960e-05,  5.6052e-45, -5.7239e-04,  2.5459e-04, -6.9299e-05,\n",
      "         5.6052e-45, -1.3461e-04, -3.5391e-04, -4.2611e-04, -2.8960e-04,\n",
      "        -5.8206e-05, -5.3557e-04, -4.0046e-04,  1.4266e-04, -1.1672e-04,\n",
      "         6.1708e-05, -7.2433e-05,  5.6052e-45,  8.0609e-05,  3.0350e-05,\n",
      "        -2.4478e-04, -3.9530e-04, -3.9923e-04,  1.5003e-04, -1.1674e-04,\n",
      "        -4.0566e-04, -2.2148e-04, -1.7605e-04,  5.5603e-05,  5.6052e-45,\n",
      "        -4.6937e-04, -1.5312e-04, -8.0195e-05, -6.1248e-04,  1.2397e-03,\n",
      "        -3.4760e-04,  4.7676e-04, -8.3114e-04,  1.5772e-04, -5.7380e-04,\n",
      "         8.3042e-04, -3.3689e-04, -1.4286e-04, -6.5525e-05, -3.0277e-04,\n",
      "         1.6232e-04, -2.0997e-05, -8.1126e-04, -2.9887e-04, -1.4059e-04,\n",
      "         3.5674e-04,  1.1958e-03, -3.2740e-05, -6.0507e-04,  9.9207e-04,\n",
      "         6.6093e-04, -5.9940e-05, -8.5976e-05,  8.1828e-05, -2.0257e-04,\n",
      "         9.4781e-05, -2.0304e-04, -6.2894e-04, -7.9188e-05, -8.6745e-05,\n",
      "        -7.0488e-05,  3.1963e-04,  1.4336e-04,  5.4682e-05,  3.2467e-04,\n",
      "        -2.9297e-04,  3.2670e-04, -9.8661e-05, -4.5914e-04, -6.9535e-04,\n",
      "         3.5551e-05, -2.5662e-04, -5.2119e-05, -3.5114e-04,  1.1800e-03,\n",
      "        -2.2447e-04,  9.0667e-05, -1.5543e-04, -8.3504e-05,  4.3485e-04,\n",
      "         1.5268e-04,  1.2641e-04, -4.4078e-04,  7.7417e-05,  3.3389e-04,\n",
      "        -6.5396e-04,  5.5444e-05,  3.7994e-04, -3.2182e-05,  4.4684e-04,\n",
      "         1.4326e-03,  3.4324e-04, -5.1025e-04, -1.4759e-04,  9.1598e-04,\n",
      "        -6.4661e-04,  2.6224e-04, -2.8104e-04,  5.6052e-45, -9.8983e-06,\n",
      "         3.7604e-04, -7.5765e-05, -7.1390e-05, -3.9001e-04, -5.9870e-05,\n",
      "        -1.9834e-04, -2.5663e-04, -1.2279e-04, -2.6416e-04,  5.6052e-45,\n",
      "         3.9714e-04, -2.4335e-04,  1.1624e-04, -4.1483e-04,  1.3480e-03,\n",
      "         3.9992e-04, -1.5984e-05,  2.7094e-05,  6.2636e-05,  1.4470e-04,\n",
      "        -3.0517e-04,  6.3361e-04,  5.6052e-45, -2.9740e-04,  1.9147e-04,\n",
      "         5.6526e-04,  2.8838e-04,  1.0150e-03, -3.0987e-04, -1.5613e-04,\n",
      "         5.6052e-45, -1.3427e-04,  2.0152e-04,  5.7227e-04, -1.1431e-10,\n",
      "         3.8809e-04, -4.2347e-04, -1.2154e-04,  5.5985e-05,  1.4539e-04,\n",
      "         8.1411e-04, -2.3290e-04, -4.2565e-04, -8.3923e-05,  7.6794e-05,\n",
      "         5.3035e-04, -4.9337e-04,  5.9768e-08, -8.1616e-05, -2.9865e-04,\n",
      "        -1.2140e-04,  1.7958e-04, -3.0601e-06,  4.6681e-04,  1.2305e-04,\n",
      "         2.2370e-04, -1.5708e-04,  2.7352e-04, -7.8410e-04, -1.9239e-04,\n",
      "         1.9319e-04, -3.2548e-04,  1.8929e-04, -1.2229e-04, -1.0244e-04,\n",
      "        -1.5032e-04,  8.2415e-05, -4.6566e-04,  5.6052e-45, -1.4844e-04,\n",
      "        -2.0911e-04,  8.9044e-04,  7.1320e-05, -2.3311e-04,  1.6478e-04,\n",
      "         4.5809e-04,  1.5775e-04,  2.0180e-04,  1.7697e-04, -1.6776e-04,\n",
      "        -3.5459e-05, -6.7951e-04, -4.1911e-04, -1.3053e-04,  4.9407e-04,\n",
      "        -9.9078e-05, -2.2638e-04, -1.8279e-04,  2.6588e-04,  5.3207e-04,\n",
      "        -3.1161e-04, -3.7270e-04, -4.8824e-04, -1.3595e-04, -2.8382e-04,\n",
      "        -2.2428e-04, -2.7753e-04,  2.3494e-04, -4.1097e-04, -2.4896e-04,\n",
      "        -5.7293e-04,  3.8223e-04,  1.7958e-04, -5.7173e-05, -1.5593e-05,\n",
      "        -7.5622e-04,  1.4587e-04,  1.0862e-04,  4.3060e-04,  7.1109e-04,\n",
      "        -9.8230e-05,  5.6052e-45,  1.9102e-04, -2.6046e-04, -1.4171e-04,\n",
      "        -3.1912e-04, -1.1239e-04, -1.6725e-04, -3.1169e-04,  4.8027e-04,\n",
      "         6.4222e-04,  2.6199e-04, -6.8512e-04, -3.1280e-04,  3.5122e-04,\n",
      "        -4.9357e-04, -9.8758e-05, -2.1477e-05, -3.9282e-04,  1.3607e-03,\n",
      "        -4.4052e-05, -6.0486e-04,  7.2259e-04, -4.8372e-04,  2.7416e-04,\n",
      "         1.6964e-04,  1.5257e-04, -7.9193e-04, -1.7410e-04, -5.0302e-05,\n",
      "        -2.3700e-05,  6.8891e-05, -4.4030e-04, -3.8018e-05,  3.8506e-05,\n",
      "         2.3754e-04,  1.6947e-04,  1.0687e-04,  2.0605e-04,  6.2820e-04,\n",
      "         4.9974e-04, -6.4452e-04, -3.4378e-04,  3.1974e-05, -4.6671e-04,\n",
      "         2.1772e-04,  4.6882e-04,  1.6974e-04, -2.7846e-04, -4.0184e-04,\n",
      "        -1.2828e-04, -2.2033e-04, -2.5099e-04,  2.4701e-04, -2.8723e-04,\n",
      "        -3.6378e-04, -2.2106e-04, -3.2276e-04,  3.9255e-04,  1.4598e-04,\n",
      "         3.5589e-05,  2.4516e-04, -2.4454e-05,  2.1726e-04,  2.3485e-04,\n",
      "         9.2117e-06, -4.0734e-04,  3.1869e-04,  4.5630e-05, -9.0637e-05,\n",
      "         9.9651e-06,  2.0393e-04, -6.6655e-04,  7.3106e-04, -8.6346e-06,\n",
      "        -1.2547e-04,  1.6352e-04, -1.9256e-04, -1.6255e-04, -4.6448e-04,\n",
      "         5.3755e-05, -3.9997e-04, -9.3665e-04, -2.2701e-05, -5.4485e-04,\n",
      "         5.9005e-04,  2.7729e-04,  7.4448e-04,  1.1301e-03,  1.4770e-04,\n",
      "        -6.3806e-04,  5.7502e-05,  1.3042e-04,  4.9536e-04, -2.5360e-04,\n",
      "        -3.2969e-04,  6.9148e-04, -8.1334e-05, -6.0022e-04,  6.2425e-06,\n",
      "         2.3449e-05,  4.6360e-04,  1.1511e-04, -1.7884e-04, -2.1725e-04,\n",
      "        -1.0267e-04, -1.9023e-04,  1.7072e-04, -3.1950e-05, -1.9825e-04,\n",
      "         8.9793e-05,  5.6076e-04,  4.6026e-06, -1.2974e-04, -2.9080e-04,\n",
      "        -3.1878e-04, -2.9942e-04,  2.4026e-04, -5.7952e-04, -1.0334e-04,\n",
      "        -1.8263e-04, -1.4475e-04, -1.6923e-04,  4.1306e-04, -4.0640e-04,\n",
      "        -2.8361e-04, -3.2820e-04,  5.0982e-04,  3.6402e-04, -4.9337e-04,\n",
      "         1.1799e-03,  3.4487e-05,  5.9701e-05,  1.3455e-04,  5.5307e-04,\n",
      "        -3.6567e-04,  1.0691e-04,  1.7625e-04, -5.2160e-04, -3.1630e-05,\n",
      "        -3.4815e-04,  6.6269e-04, -3.4623e-04, -3.3407e-04,  1.3195e-04,\n",
      "        -3.9835e-04, -6.2940e-05, -5.5392e-04, -5.7992e-04, -6.2288e-05,\n",
      "         5.6052e-45,  7.8760e-06,  1.4224e-03, -2.8853e-04, -9.4151e-05,\n",
      "         5.6052e-45, -2.5365e-04,  1.8464e-04,  5.6555e-04, -1.8895e-04,\n",
      "         6.5808e-05,  2.2446e-04, -3.3849e-04,  1.2367e-03,  5.4861e-04,\n",
      "        -5.4530e-04, -3.6025e-04,  1.0659e-04,  1.6784e-04,  5.6052e-45,\n",
      "         6.6121e-05, -1.1002e-04,  5.9911e-04,  4.6173e-04,  5.7882e-05,\n",
      "         5.1879e-04, -1.1445e-04,  2.2632e-04,  1.3024e-03,  1.6966e-05,\n",
      "         8.6995e-06, -1.9584e-05,  5.1778e-05,  1.5876e-04, -8.5593e-04,\n",
      "        -1.1796e-03, -2.1516e-04, -2.3224e-04,  2.1798e-04, -3.2131e-04,\n",
      "        -1.4381e-05,  1.4064e-03,  5.7718e-05,  3.9472e-04,  1.5127e-04,\n",
      "         3.2037e-04, -4.1741e-04,  1.6836e-04,  6.5597e-05,  8.0275e-05,\n",
      "        -1.2902e-04,  1.5603e-04,  5.6383e-04, -4.3883e-04, -1.5088e-04,\n",
      "         4.2083e-04, -7.3660e-05, -5.8136e-04,  1.0439e-05, -4.8804e-05,\n",
      "        -8.7978e-05,  5.6052e-45,  8.7120e-04,  5.1493e-04,  5.3359e-04,\n",
      "         4.6967e-05, -1.9581e-04, -5.6043e-05, -2.2521e-05, -5.8044e-05,\n",
      "         4.5721e-04,  4.6431e-04,  3.1426e-04, -1.6800e-04, -3.5162e-04,\n",
      "        -4.3336e-05,  6.9424e-05, -2.2787e-04,  4.8436e-05,  4.0310e-05,\n",
      "         4.7312e-04, -2.3786e-04,  1.7977e-04, -2.2077e-05,  3.3137e-05,\n",
      "         1.3588e-04,  5.6052e-45, -2.0178e-04,  3.0094e-04, -2.3142e-04,\n",
      "         4.6425e-04, -5.6119e-04,  2.5251e-04, -2.8948e-05,  9.6190e-05,\n",
      "        -1.3416e-03,  1.3755e-04, -3.0096e-04, -3.7660e-04, -6.3488e-08]), 'exp_avg_sq': tensor([1.1542e-06, 7.5134e-06, 3.4296e-06, 2.2517e-06, 3.4269e-06, 3.1566e-06,\n",
      "        2.6213e-06, 9.3654e-07, 1.8743e-06, 1.7700e-06, 3.8440e-06, 7.2175e-06,\n",
      "        4.3482e-06, 5.8506e-06, 2.4893e-06, 3.0068e-06, 1.6192e-06, 3.4615e-06,\n",
      "        5.7996e-06, 5.4084e-06, 5.2151e-06, 1.4792e-06, 1.5399e-06, 3.2206e-06,\n",
      "        2.1303e-06, 4.9283e-06, 3.0702e-06, 5.8722e-06, 5.1221e-06, 8.5208e-06,\n",
      "        3.6971e-06, 2.0969e-06, 1.5755e-06, 7.4572e-06, 2.5392e-06, 2.2606e-06,\n",
      "        1.4037e-09, 3.4143e-06, 2.2170e-06, 1.9152e-06, 5.5482e-09, 2.0197e-06,\n",
      "        1.4415e-06, 2.2558e-06, 2.8773e-06, 2.7792e-06, 5.6012e-06, 2.9082e-06,\n",
      "        1.7922e-06, 1.7068e-06, 4.3197e-06, 1.5086e-06, 2.7050e-10, 2.3523e-06,\n",
      "        2.1296e-06, 1.7878e-06, 3.9373e-06, 2.0784e-06, 2.3808e-06, 3.0913e-06,\n",
      "        2.4894e-06, 2.0101e-06, 1.7498e-06, 1.6186e-06, 2.8843e-10, 2.3797e-06,\n",
      "        1.4128e-06, 1.3537e-06, 5.8272e-06, 6.5055e-06, 2.0331e-06, 9.1134e-06,\n",
      "        2.6541e-06, 9.6685e-07, 1.8425e-06, 2.4408e-06, 5.5827e-06, 8.2576e-07,\n",
      "        2.4275e-06, 2.6399e-06, 3.2988e-06, 5.7196e-07, 2.1765e-06, 3.7094e-06,\n",
      "        1.9363e-06, 1.4605e-06, 1.0690e-05, 8.9901e-07, 4.0427e-06, 7.0231e-06,\n",
      "        3.6548e-06, 9.5654e-06, 2.2856e-06, 2.8103e-06, 1.4185e-06, 3.7220e-06,\n",
      "        3.2328e-06, 2.0166e-06, 1.5838e-06, 3.8056e-06, 5.4404e-06, 1.8548e-06,\n",
      "        1.4104e-06, 1.5508e-06, 2.2075e-06, 1.6862e-06, 2.1198e-06, 1.0772e-06,\n",
      "        2.5274e-06, 6.5454e-06, 1.9791e-06, 3.0177e-06, 4.3805e-06, 1.0229e-06,\n",
      "        5.3203e-06, 2.5856e-06, 3.1271e-06, 1.2493e-06, 2.5860e-06, 1.3637e-05,\n",
      "        2.5438e-06, 1.0149e-06, 4.6852e-06, 2.0670e-06, 5.4760e-06, 1.5871e-06,\n",
      "        2.9719e-06, 2.8721e-06, 3.6098e-06, 1.3072e-06, 5.6124e-06, 1.7977e-06,\n",
      "        2.9319e-06, 3.1966e-06, 4.8597e-06, 2.6495e-06, 4.0243e-06, 1.8390e-06,\n",
      "        5.5424e-10, 1.2101e-06, 1.8601e-06, 4.5093e-06, 2.8201e-06, 3.1048e-06,\n",
      "        1.4061e-06, 3.4292e-06, 5.5221e-06, 2.2356e-06, 2.1190e-06, 1.3122e-10,\n",
      "        6.4732e-06, 1.8850e-06, 2.7161e-06, 1.5543e-06, 4.6049e-06, 3.0031e-06,\n",
      "        3.4018e-06, 2.4670e-06, 4.5505e-06, 1.4993e-06, 1.6977e-06, 2.9563e-06,\n",
      "        4.6535e-09, 1.6996e-06, 2.3209e-06, 2.8197e-06, 1.9324e-06, 2.2777e-06,\n",
      "        2.2672e-06, 1.1659e-06, 1.0094e-09, 1.6537e-06, 2.8257e-06, 3.5669e-06,\n",
      "        1.2565e-08, 1.7589e-06, 6.0704e-06, 1.3843e-06, 2.3752e-06, 1.9667e-06,\n",
      "        3.8429e-06, 5.3242e-06, 1.9742e-06, 3.1110e-06, 1.3190e-06, 1.1191e-06,\n",
      "        3.1859e-06, 7.6222e-06, 2.0365e-06, 4.5345e-06, 2.3739e-06, 1.7064e-06,\n",
      "        1.4665e-06, 2.2519e-06, 2.4093e-06, 1.1632e-06, 2.7234e-06, 2.4753e-06,\n",
      "        2.4357e-06, 1.5807e-06, 2.1220e-06, 2.0600e-06, 2.4327e-06, 2.5061e-06,\n",
      "        2.5389e-06, 2.2847e-06, 6.6737e-06, 1.2571e-06, 1.5980e-09, 3.0742e-06,\n",
      "        4.8469e-06, 2.0462e-06, 2.1980e-06, 9.2128e-08, 2.4867e-06, 2.0259e-06,\n",
      "        8.0397e-07, 3.7559e-06, 1.2953e-06, 2.6424e-06, 2.0213e-06, 3.7920e-06,\n",
      "        1.8660e-06, 1.5649e-06, 1.5127e-06, 2.7837e-06, 1.3614e-06, 3.2817e-06,\n",
      "        3.5079e-06, 4.2751e-06, 1.2929e-06, 2.1444e-06, 2.2276e-06, 2.7869e-06,\n",
      "        1.5375e-06, 1.4202e-06, 1.8524e-06, 1.6746e-06, 2.9134e-06, 2.9765e-06,\n",
      "        4.1828e-06, 1.6444e-06, 3.4834e-06, 2.0281e-06, 3.0170e-06, 2.6868e-06,\n",
      "        2.5620e-06, 7.5419e-07, 3.4828e-06, 2.4128e-06, 1.3564e-06, 6.3689e-10,\n",
      "        1.1211e-06, 3.8840e-06, 2.1847e-06, 1.9858e-06, 4.1128e-06, 1.4301e-06,\n",
      "        5.9801e-06, 1.9660e-06, 2.8070e-06, 2.3946e-06, 4.9546e-06, 2.3960e-06,\n",
      "        1.0785e-06, 2.9498e-06, 1.2364e-06, 1.2753e-06, 2.9278e-06, 4.3996e-06,\n",
      "        1.4955e-06, 4.4774e-06, 3.7180e-06, 1.4520e-06, 2.2973e-06, 2.0809e-06,\n",
      "        1.0861e-06, 3.0227e-06, 2.2300e-06, 1.8248e-06, 2.4097e-06, 3.3698e-06,\n",
      "        3.6846e-06, 1.7053e-06, 1.9447e-06, 1.1601e-06, 2.2732e-06, 1.7450e-06,\n",
      "        1.1675e-06, 1.8432e-06, 2.1558e-06, 2.8244e-06, 1.1489e-05, 1.8211e-06,\n",
      "        2.3673e-06, 1.5039e-06, 1.6411e-06, 6.5268e-06, 2.3036e-06, 1.2246e-05,\n",
      "        2.5884e-06, 3.0021e-06, 3.6557e-06, 2.4679e-06, 1.5871e-06, 3.0297e-06,\n",
      "        1.8062e-06, 3.3709e-06, 2.3413e-06, 6.5083e-07, 1.3467e-06, 3.6919e-06,\n",
      "        5.4279e-06, 1.8021e-06, 3.4319e-06, 1.2596e-06, 2.7060e-06, 1.6181e-06,\n",
      "        1.9848e-06, 5.9966e-06, 1.9059e-06, 5.0107e-06, 1.9595e-06, 3.4542e-06,\n",
      "        1.4264e-06, 4.0753e-06, 1.9071e-06, 3.0413e-06, 1.1877e-06, 2.1824e-06,\n",
      "        1.2008e-06, 6.2137e-06, 2.8223e-06, 1.7533e-06, 3.7756e-06, 2.6278e-06,\n",
      "        2.6771e-06, 2.7966e-06, 2.2030e-06, 2.3920e-06, 5.9197e-06, 3.3085e-06,\n",
      "        1.4431e-06, 2.9601e-06, 1.0215e-06, 5.6105e-06, 4.8208e-06, 7.7928e-06,\n",
      "        6.0584e-06, 1.6643e-06, 4.6764e-06, 1.8058e-06, 3.0029e-06, 1.7691e-06,\n",
      "        1.3353e-06, 2.0078e-06, 9.9405e-07, 3.1474e-06, 1.3057e-06, 7.3771e-06,\n",
      "        1.0963e-05, 3.1937e-06, 2.8666e-08, 1.3288e-06, 5.8830e-06, 2.2279e-06,\n",
      "        1.1463e-06, 1.0647e-06, 2.6310e-06, 2.9676e-06, 2.5027e-06, 1.8330e-06,\n",
      "        2.0244e-06, 9.5450e-06, 2.2888e-06, 1.5444e-06, 1.7019e-06, 3.9025e-06,\n",
      "        1.6266e-06, 4.3801e-06, 6.2278e-06, 1.0004e-06, 1.9457e-06, 1.6045e-06,\n",
      "        4.0548e-06, 4.4049e-06, 5.2258e-06, 3.0018e-06, 1.3927e-06, 3.1348e-06,\n",
      "        2.7725e-06, 1.7846e-06, 2.5003e-06, 1.2519e-06, 2.8350e-06, 8.3734e-06,\n",
      "        2.1848e-06, 6.4082e-06, 3.2076e-06, 1.9952e-06, 4.3459e-10, 1.4257e-06,\n",
      "        7.0351e-06, 1.9021e-06, 2.3087e-06, 1.2155e-09, 2.9330e-06, 1.0775e-06,\n",
      "        1.6776e-06, 2.4171e-06, 1.7341e-06, 5.9203e-06, 1.9306e-06, 6.9412e-06,\n",
      "        3.9866e-06, 2.9505e-06, 2.1643e-06, 9.9603e-07, 4.2316e-07, 1.7709e-09,\n",
      "        2.1749e-06, 1.2012e-06, 2.2366e-06, 1.7361e-06, 1.0807e-06, 4.0569e-06,\n",
      "        3.9705e-06, 7.1115e-06, 4.3607e-06, 2.1844e-06, 2.7029e-06, 1.8354e-06,\n",
      "        2.0338e-06, 1.5470e-06, 2.7812e-06, 5.8043e-06, 2.0443e-06, 2.6439e-06,\n",
      "        2.5718e-06, 2.0208e-06, 1.0572e-06, 5.5562e-06, 1.1818e-06, 2.6456e-06,\n",
      "        1.4739e-06, 2.4821e-06, 3.0027e-06, 1.6588e-06, 2.9275e-06, 2.8961e-06,\n",
      "        1.7498e-06, 2.9557e-06, 2.7240e-06, 3.6883e-06, 1.4013e-06, 5.1806e-06,\n",
      "        1.2782e-06, 7.6670e-06, 1.8428e-06, 1.2077e-06, 2.9475e-06, 7.6135e-10,\n",
      "        3.2853e-06, 3.1958e-06, 4.6938e-06, 2.2649e-06, 1.9526e-06, 1.8496e-07,\n",
      "        5.5631e-06, 1.9141e-06, 2.8869e-06, 2.6826e-06, 4.0295e-06, 2.6292e-06,\n",
      "        1.3036e-06, 1.5141e-06, 2.2733e-06, 2.6467e-06, 4.3543e-06, 2.7198e-06,\n",
      "        3.2840e-06, 3.7044e-06, 7.0327e-06, 1.5003e-06, 2.2577e-06, 1.9222e-06,\n",
      "        5.9488e-11, 1.2876e-06, 3.1022e-06, 2.2791e-06, 1.2910e-06, 2.7860e-06,\n",
      "        1.4012e-06, 9.3999e-07, 3.8475e-06, 7.7395e-06, 1.7207e-06, 2.1196e-06,\n",
      "        1.6096e-06, 7.1888e-08])}, 2: {'step': tensor(2400.), 'exp_avg': tensor([[-3.5106e-04,  9.1840e-04,  2.2617e-03,  ...,  7.0299e-03,\n",
      "          3.0283e-05,  1.6198e-08],\n",
      "        [ 1.2555e-05, -3.8062e-03, -1.6230e-03,  ..., -7.6207e-04,\n",
      "         -6.1410e-05,  1.4492e-10],\n",
      "        [-3.4499e-04,  3.6061e-04,  2.7327e-04,  ..., -1.0239e-03,\n",
      "          1.4669e-04,  2.3628e-09],\n",
      "        ...,\n",
      "        [ 7.0368e-05, -4.6290e-05,  9.3959e-04,  ...,  7.2086e-04,\n",
      "          6.1793e-04, -2.4198e-07],\n",
      "        [ 1.6745e-04, -5.1311e-04, -1.8384e-03,  ..., -1.0385e-02,\n",
      "         -1.0357e-04,  9.5254e-09],\n",
      "        [ 4.2783e-05, -1.5039e-04, -2.5932e-03,  ..., -1.7930e-03,\n",
      "         -7.6635e-04,  9.7081e-08]]), 'exp_avg_sq': tensor([[1.8978e-05, 9.4573e-06, 5.9478e-06,  ..., 1.1354e-04, 6.3980e-06,\n",
      "         1.7185e-07],\n",
      "        [2.9683e-06, 1.4924e-04, 6.7518e-05,  ..., 8.2183e-06, 9.5889e-06,\n",
      "         5.2150e-10],\n",
      "        [4.3304e-05, 6.1606e-05, 2.1752e-05,  ..., 7.2818e-05, 1.9200e-05,\n",
      "         2.3821e-08],\n",
      "        ...,\n",
      "        [4.2692e-06, 1.3976e-04, 1.4082e-04,  ..., 2.0807e-05, 1.3139e-05,\n",
      "         1.2911e-06],\n",
      "        [1.3456e-05, 1.3923e-04, 1.1835e-04,  ..., 1.6209e-04, 4.2615e-05,\n",
      "         4.4880e-08],\n",
      "        [3.7502e-06, 5.8399e-05, 2.0557e-04,  ..., 5.9228e-05, 5.9861e-05,\n",
      "         4.8086e-07]])}, 3: {'step': tensor(2400.), 'exp_avg': tensor([ 1.3603e-03, -1.8534e-03,  1.2775e-04,  3.5651e-03,  1.8898e-06,\n",
      "         1.7506e-03, -4.4641e-04,  4.1500e-03, -4.1855e-03, -4.4704e-03]), 'exp_avg_sq': tensor([6.2932e-05, 7.0851e-05, 1.1943e-04, 1.6222e-04, 1.1319e-04, 1.3243e-04,\n",
      "        6.3671e-05, 1.1065e-04, 1.7519e-04, 1.7388e-04])}}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3]}]}\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())\n",
    "# Enregistre l'état par le biais d'un dictionnaire: \n",
    "\n",
    "checkpoint = {\n",
    "    \"epoch\": 90,\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optim_state\": optimizer.state_dict()\n",
    "}\n",
    "torch.save(checkpoint, \"save\\checkpoint.pth\")\n",
    "\n",
    "# load\n",
    "loaded_checkpoint = torch.load(\"save\\checkpoint.pth\")\n",
    "epoch = loaded_checkpoint[\"epoch\"]\n",
    "\n",
    "model = NeuralNet(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_classes=num_classes\n",
    "    )\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optim_state\"])\n",
    "\n",
    "print(optimizer.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intégration du DashBoard et de Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un nouveau journal pour la comparaison des run en fonction du learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_writer_Summary = \"runs_variation_lr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation dans un dictionnaire l'ensemble des méthodes de déterminations du learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout d'une règle d'évolution du learning rate\n",
    "dict_method_learning_rate = {}\n",
    "\n",
    "lambda_lr_cst = lambda epoch: learning_rate\n",
    "lmb_schedular_cst = lambda optimizer: lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr_cst)\n",
    "dict_method_learning_rate['cst'] = lmb_schedular_cst\n",
    "lambda_lr_puissance = lambda epoch: 0.9 ** epoch\n",
    "lmb_schedular_lambda = lambda optimizer: lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr_puissance)\n",
    "dict_method_learning_rate['lambda'] = lmb_schedular_lambda\n",
    "lmb_schedular_Mult = lambda optimizer: lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda coeff_mul: 0.95)\n",
    "dict_method_learning_rate['Mult'] = lmb_schedular_Mult\n",
    "lmb_schedular_Step = lambda optimizer: lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "dict_method_learning_rate['Step'] = lmb_schedular_Step\n",
    "lmb_schedular_Exp = lambda optimizer: lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "dict_method_learning_rate['Exp'] = lmb_schedular_Exp\n",
    "lmb_schedular_OneCycle = lambda optimizer: lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=2)\n",
    "dict_method_learning_rate['OneCycle'] = lmb_schedular_OneCycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: cst\n",
      "Epoch [1/2], Step [100/1200], Loss: 2.3213\n",
      "Epoch [1/2], Step [200/1200], Loss: 2.3040\n",
      "Epoch [1/2], Step [300/1200], Loss: 2.3036\n",
      "Epoch [1/2], Step [400/1200], Loss: 2.3031\n",
      "Epoch [1/2], Step [500/1200], Loss: 2.2801\n",
      "Epoch [1/2], Step [600/1200], Loss: 2.2563\n",
      "Epoch [1/2], Step [700/1200], Loss: 2.2473\n",
      "Epoch [1/2], Step [800/1200], Loss: 2.2476\n",
      "Epoch [1/2], Step [900/1200], Loss: 2.2361\n",
      "Epoch [1/2], Step [1000/1200], Loss: 2.2327\n",
      "Epoch [1/2], Step [1100/1200], Loss: 2.2030\n",
      "Epoch [1/2], Step [1200/1200], Loss: 2.2122\n",
      "Epoch [2/2], Step [100/1200], Loss: 2.2221\n",
      "Epoch [2/2], Step [200/1200], Loss: 2.1758\n",
      "Epoch [2/2], Step [300/1200], Loss: 2.2018\n",
      "Epoch [2/2], Step [400/1200], Loss: 2.1534\n",
      "Epoch [2/2], Step [500/1200], Loss: 2.1810\n",
      "Epoch [2/2], Step [600/1200], Loss: 2.1425\n",
      "Epoch [2/2], Step [700/1200], Loss: 2.1435\n",
      "Epoch [2/2], Step [800/1200], Loss: 2.1661\n",
      "Epoch [2/2], Step [900/1200], Loss: 2.1696\n",
      "Epoch [2/2], Step [1000/1200], Loss: 2.1112\n",
      "Epoch [2/2], Step [1100/1200], Loss: 2.1133\n",
      "Epoch [2/2], Step [1200/1200], Loss: 2.1193\n",
      "start: lambda\n",
      "Epoch [1/2], Step [100/1200], Loss: 1.4275\n",
      "Epoch [1/2], Step [200/1200], Loss: 1.5555\n",
      "Epoch [1/2], Step [300/1200], Loss: 1.5479\n",
      "Epoch [1/2], Step [400/1200], Loss: 1.3940\n",
      "Epoch [1/2], Step [500/1200], Loss: 1.5426\n",
      "Epoch [1/2], Step [600/1200], Loss: 1.4101\n",
      "Epoch [1/2], Step [700/1200], Loss: 1.5369\n",
      "Epoch [1/2], Step [800/1200], Loss: 1.4619\n",
      "Epoch [1/2], Step [900/1200], Loss: 1.6502\n",
      "Epoch [1/2], Step [1000/1200], Loss: 1.5057\n",
      "Epoch [1/2], Step [1100/1200], Loss: 1.4678\n",
      "Epoch [1/2], Step [1200/1200], Loss: 1.3938\n",
      "Epoch [2/2], Step [100/1200], Loss: 1.4993\n",
      "Epoch [2/2], Step [200/1200], Loss: 1.5140\n",
      "Epoch [2/2], Step [300/1200], Loss: 1.4700\n",
      "Epoch [2/2], Step [400/1200], Loss: 1.4531\n",
      "Epoch [2/2], Step [500/1200], Loss: 1.4814\n",
      "Epoch [2/2], Step [600/1200], Loss: 1.5116\n",
      "Epoch [2/2], Step [700/1200], Loss: 1.4161\n",
      "Epoch [2/2], Step [800/1200], Loss: 1.4348\n",
      "Epoch [2/2], Step [900/1200], Loss: 1.5285\n",
      "Epoch [2/2], Step [1000/1200], Loss: 1.4218\n",
      "Epoch [2/2], Step [1100/1200], Loss: 1.6944\n",
      "Epoch [2/2], Step [1200/1200], Loss: 1.4018\n",
      "start: Mult\n",
      "Epoch [1/2], Step [100/1200], Loss: 0.9649\n",
      "Epoch [1/2], Step [200/1200], Loss: 0.9148\n",
      "Epoch [1/2], Step [300/1200], Loss: 1.0021\n",
      "Epoch [1/2], Step [400/1200], Loss: 0.9370\n",
      "Epoch [1/2], Step [500/1200], Loss: 0.7968\n",
      "Epoch [1/2], Step [600/1200], Loss: 0.9801\n",
      "Epoch [1/2], Step [700/1200], Loss: 0.8414\n",
      "Epoch [1/2], Step [800/1200], Loss: 0.8518\n",
      "Epoch [1/2], Step [900/1200], Loss: 0.8816\n",
      "Epoch [1/2], Step [1000/1200], Loss: 1.0034\n",
      "Epoch [1/2], Step [1100/1200], Loss: 0.9618\n",
      "Epoch [1/2], Step [1200/1200], Loss: 0.9699\n",
      "Epoch [2/2], Step [100/1200], Loss: 0.9149\n",
      "Epoch [2/2], Step [200/1200], Loss: 1.0727\n",
      "Epoch [2/2], Step [300/1200], Loss: 1.0439\n",
      "Epoch [2/2], Step [400/1200], Loss: 0.7769\n",
      "Epoch [2/2], Step [500/1200], Loss: 0.9787\n",
      "Epoch [2/2], Step [600/1200], Loss: 1.1095\n",
      "Epoch [2/2], Step [700/1200], Loss: 1.0201\n",
      "Epoch [2/2], Step [800/1200], Loss: 0.9601\n",
      "Epoch [2/2], Step [900/1200], Loss: 1.0276\n",
      "Epoch [2/2], Step [1000/1200], Loss: 0.9105\n",
      "Epoch [2/2], Step [1100/1200], Loss: 1.0465\n",
      "Epoch [2/2], Step [1200/1200], Loss: 0.9808\n",
      "start: Step\n",
      "Epoch [1/2], Step [100/1200], Loss: 2.1179\n",
      "Epoch [1/2], Step [200/1200], Loss: 2.0859\n",
      "Epoch [1/2], Step [300/1200], Loss: 2.1649\n",
      "Epoch [1/2], Step [400/1200], Loss: 2.1621\n",
      "Epoch [1/2], Step [500/1200], Loss: 2.1691\n",
      "Epoch [1/2], Step [600/1200], Loss: 2.1179\n",
      "Epoch [1/2], Step [700/1200], Loss: 2.1442\n",
      "Epoch [1/2], Step [800/1200], Loss: 2.1564\n",
      "Epoch [1/2], Step [900/1200], Loss: 2.1831\n",
      "Epoch [1/2], Step [1000/1200], Loss: 2.1612\n",
      "Epoch [1/2], Step [1100/1200], Loss: 2.1269\n",
      "Epoch [1/2], Step [1200/1200], Loss: 2.1137\n",
      "Epoch [2/2], Step [100/1200], Loss: 2.1960\n",
      "Epoch [2/2], Step [200/1200], Loss: 2.2008\n",
      "Epoch [2/2], Step [300/1200], Loss: 2.1833\n",
      "Epoch [2/2], Step [400/1200], Loss: 2.1643\n",
      "Epoch [2/2], Step [500/1200], Loss: 2.1543\n",
      "Epoch [2/2], Step [600/1200], Loss: 2.2050\n",
      "Epoch [2/2], Step [700/1200], Loss: 2.1189\n",
      "Epoch [2/2], Step [800/1200], Loss: 2.1438\n",
      "Epoch [2/2], Step [900/1200], Loss: 2.1442\n",
      "Epoch [2/2], Step [1000/1200], Loss: 2.1706\n",
      "Epoch [2/2], Step [1100/1200], Loss: 2.1475\n",
      "Epoch [2/2], Step [1200/1200], Loss: 2.1325\n",
      "start: Exp\n",
      "Epoch [1/2], Step [100/1200], Loss: 2.2784\n",
      "Epoch [1/2], Step [200/1200], Loss: 2.2994\n",
      "Epoch [1/2], Step [300/1200], Loss: 2.2370\n",
      "Epoch [1/2], Step [400/1200], Loss: 2.2740\n",
      "Epoch [1/2], Step [500/1200], Loss: 2.2786\n",
      "Epoch [1/2], Step [600/1200], Loss: 2.2584\n",
      "Epoch [1/2], Step [700/1200], Loss: 2.1941\n",
      "Epoch [1/2], Step [800/1200], Loss: 2.2017\n",
      "Epoch [1/2], Step [900/1200], Loss: 2.2888\n",
      "Epoch [1/2], Step [1000/1200], Loss: 2.2758\n",
      "Epoch [1/2], Step [1100/1200], Loss: 2.2548\n",
      "Epoch [1/2], Step [1200/1200], Loss: 2.2742\n",
      "Epoch [2/2], Step [100/1200], Loss: 2.2563\n",
      "Epoch [2/2], Step [200/1200], Loss: 2.2816\n",
      "Epoch [2/2], Step [300/1200], Loss: 2.2695\n",
      "Epoch [2/2], Step [400/1200], Loss: 2.2454\n",
      "Epoch [2/2], Step [500/1200], Loss: 2.2330\n",
      "Epoch [2/2], Step [600/1200], Loss: 2.2945\n",
      "Epoch [2/2], Step [700/1200], Loss: 2.2842\n",
      "Epoch [2/2], Step [800/1200], Loss: 2.2629\n",
      "Epoch [2/2], Step [900/1200], Loss: 2.2869\n",
      "Epoch [2/2], Step [1000/1200], Loss: 2.2763\n",
      "Epoch [2/2], Step [1100/1200], Loss: 2.3031\n",
      "Epoch [2/2], Step [1200/1200], Loss: 2.2247\n",
      "start: OneCycle\n",
      "Epoch [1/2], Step [100/1200], Loss: 0.5438\n",
      "Epoch [1/2], Step [200/1200], Loss: 0.3845\n",
      "Epoch [1/2], Step [300/1200], Loss: 0.5342\n",
      "Epoch [1/2], Step [400/1200], Loss: 0.4051\n",
      "Epoch [1/2], Step [500/1200], Loss: 0.9126\n",
      "Epoch [1/2], Step [600/1200], Loss: 0.8304\n",
      "Epoch [1/2], Step [700/1200], Loss: 1.3523\n",
      "Epoch [1/2], Step [800/1200], Loss: 1.4159\n",
      "Epoch [1/2], Step [900/1200], Loss: 0.6066\n",
      "Epoch [1/2], Step [1000/1200], Loss: 2.6685\n",
      "Epoch [1/2], Step [1100/1200], Loss: 0.7078\n",
      "Epoch [1/2], Step [1200/1200], Loss: 1.0260\n",
      "Epoch [2/2], Step [100/1200], Loss: 0.8289\n",
      "Epoch [2/2], Step [200/1200], Loss: 0.8321\n",
      "Epoch [2/2], Step [300/1200], Loss: 1.4606\n",
      "Epoch [2/2], Step [400/1200], Loss: 1.1175\n",
      "Epoch [2/2], Step [500/1200], Loss: 0.5755\n",
      "Epoch [2/2], Step [600/1200], Loss: 0.2691\n",
      "Epoch [2/2], Step [700/1200], Loss: 1.1106\n",
      "Epoch [2/2], Step [800/1200], Loss: 0.3870\n",
      "Epoch [2/2], Step [900/1200], Loss: 0.4576\n",
      "Epoch [2/2], Step [1000/1200], Loss: 0.3196\n",
      "Epoch [2/2], Step [1100/1200], Loss: 0.4590\n",
      "Epoch [2/2], Step [1200/1200], Loss: 0.4980\n"
     ]
    }
   ],
   "source": [
    "# Pour l'affichage\n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "\n",
    "# dict of model after training\n",
    "dict_model = {}\n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for name_lr_model, lmb_lr_model in dict_method_learning_rate.items():\n",
    "    print(f\"start: {name_lr_model}\")\n",
    "    dict_model[name_lr_model] = {}\n",
    "    model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "    # Loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    writer = SummaryWriter(os.path.join(path_writer_Summary, name_lr_model))\n",
    "    schedular = lmb_lr_model(optimizer)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # origin shape: [100, 1, 28, 28]\n",
    "            # resized: [100, 784]\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            schedular.step()\n",
    "\n",
    "            # Compute the metrique pour l'affichage\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "                # Ajoute à l'affichage\n",
    "                writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps + i)\n",
    "                writer.add_scalar('accuracy', running_correct / 100, epoch * n_total_steps + i)\n",
    "                writer.add_scalar(\"learning rate\", schedular.get_last_lr()[0], epoch * n_total_steps + i)\n",
    "\n",
    "                running_correct = 0\n",
    "                running_loss = 0\n",
    "    writer.close()\n",
    "    dict_model[name_lr_model]['model'] = model\n",
    "    dict_model[name_lr_model]['writer'] = writer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase d'entrainement pour l'ensemble des jeu de données avec des learning rates différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network with the learning rate methode :cst on the 10000 test images: 65.11 %\n",
      "Accuracy of the network with the learning rate methode :lambda on the 10000 test images: 72.99 %\n",
      "Accuracy of the network with the learning rate methode :Mult on the 10000 test images: 83.0 %\n",
      "Accuracy of the network with the learning rate methode :Step on the 10000 test images: 29.32 %\n",
      "Accuracy of the network with the learning rate methode :Exp on the 10000 test images: 19.21 %\n",
      "Accuracy of the network with the learning rate methode :OneCycle on the 10000 test images: 87.07 %\n"
     ]
    }
   ],
   "source": [
    "for model_with_method_lr_specific, dict_model in dict_model.items():\n",
    "    model = dict_model['model']\n",
    "    writer = dict_model['writer']\n",
    "    # Affichage\n",
    "    tags = []\n",
    "    preds = []\n",
    "    # Test the model\n",
    "    # Durant la phase de teste le gradient n'a pas besoin des calculer car les poids ne sont pas réactualisés\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            class_prediction = [F.softmax(output, dim=0) for output in outputs]\n",
    "            preds.append(class_prediction)\n",
    "            tags.append(predicted)\n",
    "\n",
    "        preds = torch.cat([torch.stack(batch) for batch in preds])\n",
    "        tags = torch.cat(tags)\n",
    "\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(\n",
    "            'Accuracy of the network with the learning rate methode :' +\\\n",
    "            f'{model_with_method_lr_specific} on the 10000 test images: {acc} %'\n",
    "            )\n",
    "\n",
    "        classes = range(10)\n",
    "        for i in classes:\n",
    "            tags_i = tags == i\n",
    "            preds_i = preds[:, i]\n",
    "            writer.add_pr_curve(str(i), tags_i, preds_i, global_step=0)\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous êtres sur Vs Code \n",
    "- Crtl + shift + P \n",
    "- rechercher la commande  Python: Launch Tensorboard\n",
    "- Choissiser le dossier contenant les différentes informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 23732), started 0:12:09 ago. (Use '!kill 23732' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-13930839c1e70db9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-13930839c1e70db9\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs_variation_lr --host localhost --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01823324149839145"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1903632 / 1938986 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
