{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple utilisation PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch c'est quoi ?\n",
    "\n",
    "PyTorch est une bibliothèque qui permet la création et la maintenance des Réseaux de Neurones (RN).\n",
    "PyTorch est développer par Meta. Il existe d'autre Frameword de travail sur la création de Neurone comme son homologue Tensor Flows, développer par Google.\n",
    "\n",
    "En pratique \n",
    "L'une des particularités de PyTorch est de représenter les données sous forme de matrices à plusieurs dimensions appelées tenseurs. Ces derniers stockent les entrées des architectures de Deep Learning, les paramètres des couches cachées et bien évidemment les prédictions. La représentation des RN se fait par le moyen de graphique.\n",
    "\n",
    "Les avantages de PyTorch sont :\n",
    "- son fonctionnement dynamique\n",
    "- sa simplicité de vérification \n",
    "- sa souplesse\n",
    "- sa ressemblance avec la syntaxe de python\n",
    "- sa facilité à débugger\n",
    "\n",
    "Il peut maintenant même être utilisé en production même si on lui préferra Tensor FLows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation de l'ensemble des bibliothèques \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les Tensors : les opérations de bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6122, 0.8509]) tensor([[0., 0.],\n",
      "        [0., 0.]]) tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64) tensor([3.0000, 1.2000])\n"
     ]
    }
   ],
   "source": [
    "# Quelques commandes sur les tensor\n",
    "zeros_x = torch.zeros(2, 2) # Remplie un tensor de valeur nulle (premier argument donne la taille du premier colonne et le deuxième argument le second)\n",
    "one_x = torch.ones(2, 2, dtype=torch.float64) #définie un tensor remplie de 1. La taille est [2 x 2]\n",
    "rd_x = torch.rand(2) # Créer un tensor de taille 2 avec des variable uniforme\n",
    "tensor_x = torch.tensor([3, 1.2]) #Créer un tensor définie par l'utililsateur\n",
    "print(rd_x, zeros_x, one_x, tensor_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9177, 1.1447, 0.4966],\n",
      "        [0.7401, 1.2687, 1.4620]])\n",
      "tensor([[0.9177, 1.1447, 0.4966],\n",
      "        [0.7401, 1.2687, 1.4620]])\n",
      "tensor([[0.8812, 0.4556, 0.1966],\n",
      "        [0.3368, 0.8876, 0.8452]])\n",
      "tensor([[0.0336, 0.7889, 0.1490],\n",
      "        [0.2985, 0.4835, 0.9019]])\n",
      "tensor([[25.0845,  1.6611,  1.6552],\n",
      "        [ 1.8351,  3.3293,  2.3700]])\n"
     ]
    }
   ],
   "source": [
    "# Opération simple entre tensor\n",
    "tensor_x = torch.rand(2, 3)\n",
    "tensor_y = torch.rand(2, 3)\n",
    "torch_z = tensor_x.add(tensor_y)\n",
    "print(torch_z)\n",
    "print(tensor_x.add_(tensor_y)) # _ mean inplace\n",
    "\n",
    "torch_z = tensor_x.sub(tensor_y)\n",
    "print(torch_z)\n",
    "torch_z = tensor_x.mul(tensor_y)\n",
    "print(torch_z)\n",
    "torch_z = tensor_x.div(tensor_y)\n",
    "print(torch_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5436, 0.5803, 0.5348, 0.0455, 0.0957, 0.8073])\n",
      "tensor([0.2768, 0.5803, 0.8490])\n",
      "tensor([0.3810, 0.5436, 0.9971, 0.2768, 0.5803, 0.8490, 0.9844, 0.5348, 0.9424,\n",
      "        0.6810, 0.0455, 0.1542, 0.8222, 0.0957, 0.2174, 0.7333, 0.8073, 0.2220])\n",
      "tensor([[0.3810, 0.5436],\n",
      "        [0.9971, 0.2768],\n",
      "        [0.5803, 0.8490],\n",
      "        [0.9844, 0.5348],\n",
      "        [0.9424, 0.6810],\n",
      "        [0.0455, 0.1542],\n",
      "        [0.8222, 0.0957],\n",
      "        [0.2174, 0.7333],\n",
      "        [0.8073, 0.2220]])\n",
      "tensor([[0.3810, 0.5436, 0.9971],\n",
      "        [0.2768, 0.5803, 0.8490],\n",
      "        [0.9844, 0.5348, 0.9424],\n",
      "        [0.6810, 0.0455, 0.1542],\n",
      "        [0.8222, 0.0957, 0.2174],\n",
      "        [0.7333, 0.8073, 0.2220]])\n"
     ]
    }
   ],
   "source": [
    "# Opération de selection et de visualisation\n",
    "tensor_x = torch.rand(6, 3)\n",
    "print(tensor_x[:, 1]) # toutes les lignes sont selectionnées pour la 2ème colonne\n",
    "print(tensor_x[1, :]) # toutes les colonnes sont selectionnées pour la 2ème lignes\n",
    "print(tensor_x.view(18)) # flatten the tensor matrix\n",
    "print(tensor_x.view(-1, 2)) # resize the tensor\n",
    "print(tensor_x.view(-1, 3)) # resize the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "tensor([1, 2, 3, 4], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy to tensor\n",
    "array = np.array([1, 2, 3, 4])\n",
    "print(array)\n",
    "tensor_from_array = torch.from_numpy(array)\n",
    "print(tensor_from_array)\n",
    "\n",
    "# array and tensor_from_array have the same pointer of the object\n",
    "array += 1\n",
    "print(array)\n",
    "print(tensor_from_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To gain time you can go to calculate in a particular device\n",
    "# If cuda is avalaible then you can compute with the ship IU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(5, device=device)\n",
    "    y = torch.ones(5)\n",
    "    y = y.to(device) # go to device\n",
    "    z = x + y\n",
    "    z = z.to(\"cpu\") # back to the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Si le calcul du gradient est à prévoir le paametre required_grad permet d'accélerer les opérations.\n",
    "torch_x = torch.ones(5, requires_grad=True)\n",
    "torch_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd : Bibliothèque pour calculer les Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch crée un graphique lors de la création des tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0735, 1.8457, 1.0720], grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3050, -0.5697, -1.9896], grad_fn=<MulBackward0>)\n",
      "tensor(-0.7514, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch_x = torch.randn(3, requires_grad=True)\n",
    "y = torch_x + 2\n",
    "print(y)\n",
    "z = y*torch_x*2\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward : Calcul le gradient spécifiquement définie dans le graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4981,  0.2800,  1.9867])\n",
      "tensor([-1.3981,  1.2800, 11.9867])\n"
     ]
    }
   ],
   "source": [
    "z.backward() # definie le calcul dz/dx only work for scalar value\n",
    "print(torch_x.grad)\n",
    "\n",
    "v = torch.tensor([0.1, 1, 10], dtype=torch.float32, requires_grad=True) # must be of the same size as y because we use the chain rule\n",
    "y.backward(v) # jaconbien vector\n",
    "print(torch_x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0735, 1.8457, 1.0720], grad_fn=<AddBackward0>)\n",
      "tensor([2.0735, 1.8457, 1.0720])\n",
      "tensor([ 0.0735, -0.1543, -0.9280], requires_grad=True)\n",
      "tensor([ 0.0735, -0.1543, -0.9280])\n"
     ]
    }
   ],
   "source": [
    "# change les parametres du tensors etpermet de ne pas inclure la structure du graphique\n",
    "\n",
    "y = torch_x + 2\n",
    "print(y)\n",
    "with torch.no_grad(): # supprime la construction du graphique\n",
    "    y = torch_x + 2\n",
    "    print(y)\n",
    "\n",
    "print(torch_x)\n",
    "torch_x.requires_grad_(False) # supprime l'argument rquires_grad (méthode emplace)\n",
    "print(torch_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de l'utilisation du backward et du grad au sein d'un modèle fictif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.])\n",
      "tensor([6., 6., 6.])\n",
      "tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Rénitialise le gradient après un calcul est une étape importante\n",
    "\n",
    "weight = torch.ones(3, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weight * 3).sum() # Opération du modèle\n",
    "\n",
    "    model_output.backward() # détermine le gradient\n",
    "\n",
    "    print(weight.grad) # affiche le gradient\n",
    "\n",
    "    if epoch == 1:\n",
    "        weight.grad.zero_() # rénitialise le gradient sinon l'information est conservée d'une époch a une autre\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Une autre pratique peux être l'utilisation d'une méthode de descente de gradient stochastique \n",
    "weights = [torch.ones(4, requires_grad=True), torch.zeros(4, requires_grad=True)]\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01) # lr désigne le taux d'apprentissage (learning rate)\n",
    "optimizer.step() #réalise l'opération d'optimisation\n",
    "optimizer.zero_grad() # rénitialise le gradient avant de réaliser l'étape prochaine\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Chain Rule](Images\\PyTorch_Tutorial_04_Backpropagation_Theory_With_Example_YouTube_Chain_Rule.png)\n",
    "![Computation Graphique](Images\\PyTorch_Tutorial_04_Backpropagation_Theory_With_Example_YouTube_Computation_Graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# Application dans un cas pratique\n",
    "# Input of model\n",
    "tensor_x = torch.tensor(1.0)\n",
    "tensor_y = torch.tensor(2.0)\n",
    "\n",
    "# Weigth of model\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "y_hat = w * tensor_x\n",
    "s = y_hat - tensor_y\n",
    "loss = (s) ** 2 # Ecart quadratic is the loss function considerated\n",
    "print(loss)\n",
    "\n",
    "#backward \n",
    "loss.backward() # réalise la propagation à l'envers\n",
    "print(w.grad) # preuve mise à jour de la variable grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que vaut le gradient ?\n",
    "\n",
    "Réponse:\n",
    "- dLoss/ds = 2 * s\n",
    "- ds/dy = 1\n",
    "- dy/dw = x\n",
    "\n",
    "*Rappel les inputs sont des parametres au vu du problème d'optimisation et non des variables.*\n",
    "\n",
    "En peut donc calculer le gradient : \n",
    "\n",
    "Les valeur des inputs sont : $x = 1, y = 2 \\text{ et } w = 1$\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial{Loss}}{\\partial{w}} &= \\frac{\\partial{Loss}}{\\partial{s}} \\cdot \\frac{\\partial{s}}{\\partial{y}} \\cdot \\frac{\\partial{y}}{\\partial{w}} \\\\\n",
    "&= (2 \\cdot -1) \\cdot (1) \\cdot (1)\\\\\n",
    "&= -2\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple Manuellement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de numpy pour calibrer un ajustement - Théorique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La prédiction avant entrainement: forward(5) = 0.000\n",
      "période 1 : w = 1.680 et loss = 4.800\n",
      "période 2 : w = 1.872 et loss = 0.768\n",
      "période 3 : w = 1.949 et loss = 0.123\n",
      "période 4 : w = 1.980 et loss = 0.020\n",
      "période 5 : w = 1.992 et loss = 0.003\n",
      "période 6 : w = 1.997 et loss = 0.001\n",
      "période 7 : w = 1.999 et loss = 0.000\n",
      "période 8 : w = 1.999 et loss = 0.000\n",
      "période 9 : w = 2.000 et loss = 0.000\n",
      "Après l'entrainement sur 10 itération, la prédiction vaut : forward(5) = 10.0\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32) # Y = 2 *X\n",
    "\n",
    "# Variable \n",
    "w = 0.0\n",
    "\n",
    "# Model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# Fonction de perte\n",
    "def loss(y, y_predict):\n",
    "    return ((y_predict - y) ** 2).mean()\n",
    "\n",
    "# Gradient\n",
    "# - MSE : 1/N * (w*x - y) ** 2\n",
    "# dJ / dw = 1/N 2x (w*x-y)\n",
    "\n",
    "def gradient(x, y, y_predict):\n",
    "    return np.dot(2*x, y_predict - y).mean()\n",
    "\n",
    "print(f'La prédiction avant entrainement: forward(5) = {forward(5):.3f}')\n",
    "\n",
    "# Entrainement\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for each_epoch in range(n_iters):\n",
    "    # prédiction\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # Calcul la fonction de perte\n",
    "    loss_coef = loss(Y, y_pred)\n",
    "\n",
    "    # calcule du gradient\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    #Mise à jour de la valeur\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if each_epoch > 0:\n",
    "        print(f\"période {each_epoch} : w = {w:.3f} et loss = {loss_coef:.3f}\")\n",
    "    \n",
    "print(f\"Après l'entrainement sur {n_iters} itération, la prédiction vaut : forward(5) = {forward(5):.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de PyTorch pour réaliser un ajustement - Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "période 0 : w = 0.300 et loss = 30.000\n",
      "période 5 : w = 1.246 et loss = 5.906\n",
      "période 10 : w = 1.665 et loss = 1.163\n",
      "période 15 : w = 1.851 et loss = 0.229\n",
      "période 20 : w = 1.934 et loss = 0.045\n",
      "période 25 : w = 1.971 et loss = 0.009\n",
      "période 30 : w = 1.987 et loss = 0.002\n",
      "période 35 : w = 1.994 et loss = 0.000\n",
      "période 40 : w = 1.997 et loss = 0.000\n",
      "période 45 : w = 1.999 et loss = 0.000\n",
      "période 50 : w = 1.999 et loss = 0.000\n",
      "Après l'entrainement sur 55 itération, la prédiction vaut : forward(5) = 10.0\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de Pytorch\n",
    "# Input\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32) # Y = 2 *X\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "n_iters = 55\n",
    "\n",
    "for each_epoch in range(n_iters):\n",
    "    # prédiction\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # Calcul la fonction de perte\n",
    "    loss_coef = loss(Y, y_pred)\n",
    "\n",
    "    # calcule du gradient Méthode approximative\n",
    "    loss_coef.backward()\n",
    "\n",
    "    #Mise à jour de la valeur\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    w.grad.zero_()\n",
    "\n",
    "    if each_epoch % 5 == 0:\n",
    "        print(f\"période {each_epoch} : w = {w:.3f} et loss = {loss_coef:.3f}\")\n",
    "    \n",
    "print(f\"Après l'entrainement sur {n_iters} itération, la prédiction vaut : forward(5) = {forward(5):.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On remarquera que pour la méthode avec numpy 10 itérations sont nécessaire pour converger vers la bonne valeur du coefficient alors que dans le cas de PyTorch, il en faut au moins 50.\n",
    "La raison est que la méthode de backward propagation est une méthode numérique approximative contrairement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthode d'élaboration d'un projet à l'aide de pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les étapes pour la réalisation d'un modèle sont les suivants:\n",
    "1) Création de l'architecture du modèle :\n",
    "    - Input\n",
    "    - Dimension des Output \n",
    "    - Forward pass\n",
    "2) Developpement de la fonction de coûts et intialiation de la méthode d'optimisation\n",
    "3) Réalisation de l'entrainement, les étapes suivantes doivent être réalisé:\n",
    "    - Calculer la prédiction (forward pass)\n",
    "    - Déterminer le gradient (backward pass)\n",
    "    - Met à jour les poids (Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Utilisation de Pytorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Input - Changement des Inputs pour que les information soient rangé par lignes (Une transposée à eu lieu par rapport à avant)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      6\u001b[0m Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m6\u001b[39m], [\u001b[38;5;241m8\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m# Y = 2 *X\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Définition d'un jeu de test :\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Utilisation de Pytorch\n",
    "# Input - Changement des Inputs pour que les information soient rangé par lignes (Une transposée à eu lieu par rapport à avant)\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32) # Y = 2 *X\n",
    "\n",
    "# Définition d'un jeu de test :\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# Définition du modèle - Généralement cette étape est à construire mais dans ce contexte de régression linéaire simple, avec une seul couche, il est possible d'utiliser directement le modèle suivant :\n",
    "n_samples, n_features = X.shape # définition de la taille des Inputs\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Parametre de training\n",
    "learning_rate = 0.0119\n",
    "n_iters = 55\n",
    "\n",
    "# Fonction d'optimisation\n",
    "loss = nn.MSELoss() #définie la méthode de loss qui est d'écart quadratique moyenne\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # définition de l'objet à calibrer\n",
    "\n",
    "for each_epoch in range(n_iters):\n",
    "    # prédiction\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # Calcul la fonction de perte\n",
    "    loss_coef = loss(Y, y_pred)\n",
    "\n",
    "    # calcule du gradient Méthode approximative\n",
    "    loss_coef.backward()\n",
    "\n",
    "    #Mise à jour de la valeur\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if each_epoch % 5 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f\"période {each_epoch} : w = {w[0][0].item():.3f} et loss = {loss_coef:.3f}, b = {b}\")\n",
    "    \n",
    "print(f\"Après l'entrainement sur {n_iters} itération, la prédiction vaut : forward(5) = {model(X_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque** :\n",
    "- La méthode d'optimisation présent dans l'optimizer est une méthode aléatoire (SGD : algorithme du gradient stochastique). Les résultats sont donc non déterministe et des écarts éxistes entre la version attendue et réel obtenue.\n",
    "- La méthode proposée est a une couche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas généralisé à N couches\n",
    "\n",
    "On applique le même code à l'exeption du modèle qui est définie par une classe contenant les différentes couches avec une méthode forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "période 0 : w = -0.240 et loss = 69.361, b = Parameter containing:\n",
      "tensor([-0.5843], requires_grad=True)\n",
      "période 5 : w = 1.225 et loss = 7.570, b = Parameter containing:\n",
      "tensor([-0.0891], requires_grad=True)\n",
      "période 10 : w = 1.709 et loss = 0.829, b = Parameter containing:\n",
      "tensor([0.0726], requires_grad=True)\n",
      "période 15 : w = 1.870 et loss = 0.094, b = Parameter containing:\n",
      "tensor([0.1242], requires_grad=True)\n",
      "période 20 : w = 1.924 et loss = 0.014, b = Parameter containing:\n",
      "tensor([0.1395], requires_grad=True)\n",
      "période 25 : w = 1.942 et loss = 0.005, b = Parameter containing:\n",
      "tensor([0.1428], requires_grad=True)\n",
      "période 30 : w = 1.948 et loss = 0.004, b = Parameter containing:\n",
      "tensor([0.1422], requires_grad=True)\n",
      "période 35 : w = 1.951 et loss = 0.003, b = Parameter containing:\n",
      "tensor([0.1403], requires_grad=True)\n",
      "période 40 : w = 1.953 et loss = 0.003, b = Parameter containing:\n",
      "tensor([0.1380], requires_grad=True)\n",
      "période 45 : w = 1.954 et loss = 0.003, b = Parameter containing:\n",
      "tensor([0.1356], requires_grad=True)\n",
      "période 50 : w = 1.955 et loss = 0.003, b = Parameter containing:\n",
      "tensor([0.1332], requires_grad=True)\n",
      "Après l'entrainement sur 55 itération, la prédiction vaut : forward(5) = 9.908\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de Pytorch\n",
    "# Input - Changement des Inputs pour que les information soient rangé par lignes (Une transposée à eu lieu par rapport à avant)\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32) # Y = 2 *X\n",
    "\n",
    "# Définition d'un jeu de test :\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# Définition du modèle - Généralement cette étape est à construire mais dans ce contexte de régression linéaire simple, avec une seul couche, il est possible d'utiliser directement le modèle suivant :\n",
    "n_samples, n_features = X.shape # définition de la taille des Inputs\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs) # Initialisation de l'objet Module\n",
    "        # Definie les couches / Defined Layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim) # 1er couche\n",
    "    \n",
    "    def forward(self, value):\n",
    "        return self.lin(value)\n",
    "\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# Parametre de training\n",
    "learning_rate = 0.0119\n",
    "n_iters = 55\n",
    "\n",
    "# Fonction d'optimisation\n",
    "loss = nn.MSELoss() #définie la méthode de loss qui est d'écart quadratique moyenne\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # définition de l'objet à calibrer\n",
    "\n",
    "for each_epoch in range(n_iters):\n",
    "    # prédiction\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # Calcul la fonction de perte\n",
    "    loss_coef = loss(Y, y_pred)\n",
    "\n",
    "    # calcule du gradient Méthode approximative\n",
    "    loss_coef.backward()\n",
    "\n",
    "    #Mise à jour de la valeur\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if each_epoch % 5 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f\"période {each_epoch} : w = {w[0][0].item():.3f} et loss = {loss_coef:.3f}, b = {b}\")\n",
    "    \n",
    "print(f\"Après l'entrainement sur {n_iters} itération, la prédiction vaut : forward(5) = {model(X_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de Classification appliquée à 2 couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary classification\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out) # activation function\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss()\n",
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiclass problem\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()  # (applies Softmax)\n",
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
